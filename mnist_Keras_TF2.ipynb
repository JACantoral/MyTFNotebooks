{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level Tensorflow model for MNIST/CIFAR10 classification\n",
    "After completing the very low level Tensorflow tutorial, we will now implement the Keras approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing the data\n",
    "As mentioned before, I will be using MNIST and CIFAR10 for this notebook. \n",
    "We can use Tensorflow-Keras with the command 'tf.keras.datasets.mnist.load_data()' and download the database online, or we can download it from:\n",
    "https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "For more detailed information on the MNIST dataset, please refer to [LeCun et al., 1998a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998. Also please see http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "In a second experiment, we will use the CIFAR10 dataset, which we will download from Alex Krizhevsky website at University of Toronto at https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "For more details of the CIFAR10, dataset please refer to https://www.cs.toronto.edu/~kriz/cifar.html or \n",
    "Krizhevsky, A., \"LearningMultipleLayersofFeaturesfromTinyImages\", 2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to use MNIST\n",
    "# # (x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data('/tf/mydata/mnist.npz')\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (60000, 28, 28)\n",
      "Train labels shape:  (60000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# It is a good idea to visualise the data we just loaded.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape:  (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0876183240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, as expected we have 60,000 training images and 10,0000 for testing. What is\n",
    "# each image shape? Let's find out\n",
    "print('Train image shape: ', x_train[1].shape)\n",
    "\n",
    "# Knowing it is a matrix shape, we can randomly show any of the numbers\n",
    "rnd_idx = np.random.randint(x_train.shape[0])\n",
    "plt.imshow(x_train[rnd_idx]) #Using astype guarantess imshow work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data are numpy arrays\n",
    "assert (isinstance(x_train, np.ndarray) and isinstance(y_train, np.ndarray)), 'No ndarray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:1].shape # In case we need to check the shape of one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras Sequential\n",
    "\n",
    "In this notebook I will show a super simple implementation using high level Keras with automatic eager execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define the architecture of our FC networks.\n",
    "# I will start with a 4-Layer vanilla (tasty) FC NN with ReLu activations, weight decay regularization (L2),\n",
    "# using the number of hidden units given in, well, hidden_units\n",
    "\n",
    "def four_layer_FC(hidden_units):\n",
    "    model = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                            tf.keras.layers.Dense(hidden_units[0], activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_units[1], activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_units[2], activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_units[3], activation='softmax')\n",
    "                            ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [100, 100, 100, 10]\n",
    "model = four_layer_FC(hidden_units)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1271 - accuracy: 0.9630\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1200 - accuracy: 0.9655\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1131 - accuracy: 0.9681\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1074 - accuracy: 0.9696\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.1015 - accuracy: 0.9708\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0966 - accuracy: 0.9725\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0918 - accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0873 - accuracy: 0.9753\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0830 - accuracy: 0.9764\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0795 - accuracy: 0.9776\n",
      "10000/1 - 0s - loss: 0.0550 - accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10119772570095956, 0.9688]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 64, epochs=10)\n",
    "\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted number is:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us test the result\n",
    "\n",
    "hidden_units = [100, 100, 50, 10]\n",
    "\n",
    "idx = np.random.randint(10000)\n",
    "\n",
    "plt.imshow(x_test[idx])\n",
    "print('The predicted number is: ', np.argmax(model.predict(x_test[idx].reshape(1,28,28))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_layer_fc2(x, hidden_units):\n",
    "    flatten = tf.keras.layers.Flatten(input=x)\n",
    "    fc1 = tf.keras.layers.Dense(input=flatten, units=hidden_units[0], activation='relu')\n",
    "    fc2 = tf.keras.layers.Dense(input=fc1, units=hidden_units[1], activation='relu')\n",
    "    fc3 = tf.keras.layers.Dense(input=fc2, units=hidden_units[2], activation='relu')\n",
    "    probs = tf.keras.layers.Dense(input=fc3, activation='softmax')\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be sure that the model produces an expected output for a given input. Thus, the following step just veryfies that running data through produces a tensor of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function to test if the inference graph produces the expected output dimensions given an input\n",
    "# in this case the output should be of shape (N, 10)\n",
    "\n",
    "def test_FC(num_samples, dataset):\n",
    "    # Let us declare some useful constants\n",
    "    if dataset == 'MNIST':\n",
    "        PIXELS = x_test.shape[1] * x_test.shape[2]\n",
    "    elif dataset == 'CIFAR10':\n",
    "        PIXELS = x_test.shape[1] * x_test.shape[2] * x_test.shape[3]\n",
    "    hidden_units = [100, 100, 100, 10]\n",
    "    \n",
    "    # Reset the default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define placeholder\n",
    "    if dataset == 'MNIST':\n",
    "        x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "    else:\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    \n",
    "    # Obtain parameters\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to the graph\n",
    "    scores = four_layer_FC(PIXELS, x, parameters, dataset)\n",
    "    \n",
    "    # Create session and run it\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        test = sess.run(scores, feed_dict={x:x_train[:num_samples]})\n",
    "        print(test.shape)\n",
    "\n",
    "# Test our model output size\n",
    "test_FC(10, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training model\n",
    "In this point we will start training our graph. For this we will use a Tensorflow session to run the inference graph and the training operations. This will look similiar to test_FC(), plus the required components to define a loss function and carry out learning step operations. For simplicity let us use vanilla Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines the complete training graph\n",
    "\n",
    "def train_FC_model(hidden_units, \n",
    "                   num_epochs=10,\n",
    "                   PIXELS=784,\n",
    "                   learning_rate=0.05,\n",
    "                   reg=0,\n",
    "                   print_every=100,\n",
    "                   minibatch_size = 64):\n",
    "    '''\n",
    "    Train Tensorflow model\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_units: List with number of hidden neurons per layer\n",
    "    - num_epochs: Integer with the number of epochs to run, an epoch is a complete pass in the whole training set\n",
    "    - PIXELS: Scalar with total number of pixels\n",
    "    - learning_rate: Float with the learning rate to use for updates, i.e. the step size towards the minimum\n",
    "    - reg: L2 regularization strength, default is set to 0 for no regularization\n",
    "    - print_every: This is a helping variable to stop during training and evaluate loss functions and accuracy\n",
    "    - minibatch_size: Integer with the number of elements in minibatch\n",
    "    \n",
    "    Outputs:\n",
    "    - updated_parameters: List with update parameters\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Reset default graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define training data placeholders using expected MNIST data dimensions\n",
    "    if dataset == 'MNIST':\n",
    "        x = tf.placeholder(tf.float32, [None, 28, 28], name = 'x_train') # Training data\n",
    "        y = tf.placeholder(tf.int32,[None, ], name='y_train') # Labels\n",
    "    elif dataset == 'CIFAR10':\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3], name = 'x_train') # Training data\n",
    "        y = tf.placeholder(tf.int32, [None, ], name = 'y_train') # Training data\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add these placeholders to graph in saver, this will allow for easy model restore\n",
    "    tf.add_to_collection('images', x)\n",
    "    tf.add_to_collection('labels', y)\n",
    "\n",
    "    # Load parameters by running the previously defined function\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to graph using the function we coded, and the parameters\n",
    "    scores = four_layer_FC(PIXELS, x, parameters, dataset)\n",
    "    \n",
    "    # Save scores to model, this is key since it will allow using it for inference after restore\n",
    "    tf.add_to_collection('scores', scores)\n",
    "    \n",
    "    # Once the scores are computed, we need to obtain our Loss. Given the data, we will use Cross entropy \n",
    "    # Using Tensorflow implementation of Cross entropy\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=y) # Each sample loss\n",
    "    data_loss = tf.reduce_mean(losses, name='loss') # take minibatch average\n",
    "    \n",
    "    # Should we want to use l2 regularization, default is zero, i.e., no regularization\n",
    "    reg_loss = tf.reduce_mean(reg * (tf.nn.l2_loss(parameters[0])\n",
    "                                     + tf.nn.l2_loss(parameters[2])\n",
    "                                     + tf.nn.l2_loss(parameters[4])\n",
    "                                     + tf.nn.l2_loss(parameters[0])))\n",
    "    \n",
    "    loss = data_loss + reg_loss # Compute total loss\n",
    "    \n",
    "    # Up to this point we have computed the complete forward pass of the data. \n",
    "    # Since, this is 'low level' Tensorflow we need to compute the gradients for the loss \n",
    "    # w.r.t. all the learnable parameters. As I said, low level but still Tensorflow. So let's use tf.gradients!\n",
    "    grad_parameters = tf.gradients(loss, parameters) # Loss gradient w.r.t. parameters\n",
    "\n",
    "    # We need to update the weights manually, for this we can use tf.assign, or tf.assing_sub\n",
    "    # Using SGD, we need to update each parameter independently, so list comprehension works well\n",
    "    updated_parameters = [tf.assign_sub(w, learning_rate * grad)\n",
    "                          for w, grad in zip(parameters, grad_parameters)]\n",
    "    \n",
    "    tf.add_to_collection('weights', updated_parameters)\n",
    "    \n",
    "    accuracies = np.zeros(10) # Helping variable to store accuracies\n",
    "    losses = np.zeros(10) # Helping variable to store accuracies\n",
    "    \n",
    "    # Create saver to save the model\n",
    "    saver = tf.train.Saver()\n",
    "    # Now, recall we have only created the graph, in order to run it and train, we need a Session\n",
    "    with tf.Session() as sess:\n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model for num_epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            print('Epoch number: ', epoch) # Allow us to see what epoch we are running\n",
    "            \n",
    "            # Run the necessary iterations given the number of minibatches, this follows the function defined\n",
    "            # at the beginning of the notebook\n",
    "            for iteration,(x_mb, y_mb) in enumerate(mini_batches(minibatch_size, x_train, y_train)):\n",
    "                # The following line calculates the loss for the minibatch.\n",
    "                # Recall TF knows what is needed to run a function and will run accordinly, e.g. scores before the\n",
    "                # loss. Also note that we only need to feed the data needed into a placeholder. In this case, we \n",
    "                # feed the minibatch training samples and labels\n",
    "                loss_mb, update_param = sess.run([loss, updated_parameters], feed_dict={x:x_mb, y:y_mb})\n",
    "                \n",
    "                # The following condition allows a sanity check by printing accuracies and loss\n",
    "                if iteration % print_every == 0:\n",
    "                    # We define this function in the next cell\n",
    "                    accuracy = compute_accuracy(sess, minibatch_size, scores, x)\n",
    "                    accuracies[int(iteration/100)]=accuracy # save current accuracy\n",
    "                    losses[int(iteration/print_every)] = loss_mb\n",
    "                    print('Iteration: %d Loss: %f Accuracy: %f Learning rate: %f'\n",
    "                          %(iteration, loss_mb, accuracy, learning_rate))\n",
    "        \n",
    "        # We could use learning rate decay. There are several conditions we could test to define when to decay.\n",
    "        # E.g. we could use it when accuracy is over e.g. 90%, or the loss stays relatively flat\n",
    "            print('Last losses std: ', np.std(losses))\n",
    "            if np.std(losses) < 0.80:\n",
    "                learning_rate = 0.95 * learning_rate\n",
    "        # Save the \n",
    "        saver.save(sess, 'checkpoint_file')\n",
    "        \n",
    "    return update_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(sess, minibatch_size, scores, x):\n",
    "    '''\n",
    "    This function computes the accuracy of the current model\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: it needs a current tf.Session to run the scores\n",
    "    - minibatch_size: The size of the mini batch to run the scores\n",
    "    - scores: TF operation to run\n",
    "    - x: test data\n",
    "    \n",
    "    Outputs:\n",
    "    - acc: Accuracy\n",
    "    \n",
    "    '''\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    for it, (xtest_mb, ytest_mb) in enumerate(mini_batches(minibatch_size, x_test, y_test)):\n",
    "        scores_test = sess.run(scores, feed_dict={x:xtest_mb})\n",
    "        y_pred = np.argmax(scores_test, axis=1)\n",
    "\n",
    "#         In case we would like to compare some elements of the predicted and ground truth arrays\n",
    "#         if it % 200 == 0:\n",
    "#             print('y_pred: ', y_pred[:10])\n",
    "#             print('y_test: ', ytest_mb[:10])\n",
    "\n",
    "        num_samples += xtest_mb.shape[0]\n",
    "        num_correct += np.sum(np.equal(y_pred, ytest_mb))\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define number of neurons (units) in each hidden layer\n",
    "hidden_units = [100, 100, 100, 10]\n",
    "\n",
    "# Train the model and save weights\n",
    "updated_parameters =  train_FC_model(hidden_units=hidden_units, \n",
    "                                     num_epochs=20,\n",
    "                                     PIXELS=PIXELS, \n",
    "                                     learning_rate=0.005,\n",
    "                                     minibatch_size=128,\n",
    "                                     reg= 0.0, \n",
    "                                     print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instruction is not needed for the tutorial but it is useful to know the variablse in the graph\n",
    "auxL = [op.name for op in tf.get_default_graph().get_operations() if op.op_def and op.op_def.name=='VariableV2']\n",
    "print(auxL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [100, 100, 100, 10]\n",
    "if dataset == 'MNIST':\n",
    "    x = tf.placeholder(tf.float32, [1, 28, 28])\n",
    "    classes = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five',\n",
    "              'Six', 'Seven', 'Eight', 'Nine']\n",
    "elif dataset == 'CIFAR10':\n",
    "    x = tf.placeholder(tf.float32, [1, 32, 32, 3])\n",
    "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "              'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "#parameters = init_four_layer_FC(784, hidden_units)\n",
    "# Add scores\n",
    "scores = four_layer_FC(PIXELS, x, updated_parameters,dataset)    \n",
    "#eval_op = tf.nn.top_k(scores)\n",
    "\n",
    "idx = np.random.randint(10000)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if dataset=='MNIST':\n",
    "        scores2 = sess.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    else:\n",
    "        scores2 = sess.run(scores, feed_dict={x:x_test[idx].reshape(1, 32, 32, 3)})\n",
    "        \n",
    "    print('The predicted class is: ', classes[np.argmax(scores2)])\n",
    "    plt.imshow(x_test[idx].astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the previously trained model\n",
    "\n",
    "So far we have trained the complete model, computed its accuracy, and validated it with random samples. However, we did the whole process in one go. Since the dataset is relatively simple this is not an issue, nonetheless this would be prohibiting for more complex and larger datasets. Since we saved our trained model, we can just load it and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    x = tf.placeholder(tf.float32, [1, 28, 28])\n",
    "    classes = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five',\n",
    "              'Six', 'Seven', 'Eight', 'Nine']\n",
    "elif dataset == 'CIFAR10':\n",
    "    x = tf.placeholder(tf.float32, [1, 32, 32, 3])\n",
    "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "              'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    # Load saved Model\n",
    "    saver = tf.train.import_meta_graph('checkpoint_file.meta')\n",
    "    saver.restore(sess2, 'checkpoint_file')\n",
    "    \n",
    "    # Load saved points\n",
    "    # Scores includes the complete model, with final parameters\n",
    "    scores = tf.get_collection('scores')[0]\n",
    "    # Saved placeholders, since we only want to run the inference graph, we only load x\n",
    "    x = tf.get_collection('images')[0]\n",
    "    \n",
    "    # Choose radom point in test data\n",
    "    idx = np.random.randint(10000)\n",
    "    plt.imshow(x_test[idx].astype(np.int32))\n",
    "    \n",
    "    # Calculate the score\n",
    "    if dataset=='MNIST':\n",
    "        scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    else:\n",
    "        scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 32, 32, 3)})\n",
    "        \n",
    "    print('The predited number is: ', classes[np.argmax(scores2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
