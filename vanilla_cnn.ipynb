{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with vanilla Convolutional Neural Network using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we built a simple 4-layer vanilla NN with MNIST. We saw, that this simple architecture achieved over 95% accuracy on the test dataset without much tunning (or any whatsoever). Thus, using Convolutional Neural Networks we should achive higher accuracy without much more effort. Then, we tried using CIFAR with basically the same simple architecture, achieving ~48% accuracy with 20 epochs, not so succesful.\n",
    "In this notebook, we will follow two different approaches to train a CNN with CIFAR10 data. First, it will be a simpler network with low level tensorflow using tf.gradients. Then, we will improve our model using tf.keras.conv2d layers, which will take care of updating parameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing the data\n",
    "We will use the CIFAR10 dataset, which we will download from Alex Krizhevsky website at University of Toronto at https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "For more details of the CIFAR10, dataset please refer to https://www.cs.toronto.edu/~kriz/cifar.html or \n",
    "Krizhevsky, A., \"LearningMultipleLayersofFeaturesfromTinyImages\", 2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
    "Since, here we will only use CIFAR10 dataset, we don't need to load MNIST.\n",
    "Initially the notebook will follow a very similar style to that previously completed, likewise we will share some functions like the accuracy check and minibatch.\n",
    "\n",
    "Furthermore, to have a cleaner notebook, we will not include all the sanity checks we used in the previous notebooks, e.g. we will not check all the data types and shapes, since we already did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This functions load CIFAR 10 data (from binary files inside cifar-10-python.tar.gz). Notice that the files \n",
    "    have been 'untar-ed' manually into folder specified in path. This is convenient, since it is not ideal to \n",
    "    download the data everytime we run the notebook, particularly because CIFAR10 tar file is ~170MB.\n",
    "\n",
    "    These two functions are based on two function provided in data_utils.py in Stanford CNN for Visual \n",
    "    Recognition CS231n in (https://cs231n.github.io/assignments2019/assignment1/), and by Tensorflow cifar10.py as\n",
    "    in: https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/datasets/cifar10.py\n",
    "    and https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/datasets/cifar.py\n",
    "\n",
    "     Inputs:\n",
    "     - path: Path to CIFAR10 'untar-ed' files\n",
    "     \n",
    "     Outputs:\n",
    "     - (x_train, y_train), (x_test, y_test): Two tuples with numpy arrays containing the train and test data \n",
    "'''\n",
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "    \n",
    "def load_CIFAR10_data(path):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(path, 'data_batch_%d' % (b, ))\n",
    "        x, y = load_CIFAR_batch(f)\n",
    "        data.append(x)\n",
    "        labels.append(y)\n",
    "    x_train = np.concatenate(data)\n",
    "    y_train = np.concatenate(labels)\n",
    "    del x, y\n",
    "    x_test, y_test = load_CIFAR_batch(os.path.join(path, 'test_batch'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Using encoding 'latin1' prevents errors with objects that may have been pickled with Python2\n",
    "        d = pickle.load(f, encoding='latin1')    \n",
    "        data = d['data']\n",
    "        labels = d['labels']\n",
    "        data = data.reshape(10000, 3, 32, 32).transpose(0,2,3,1)\n",
    "        labels = np.array(labels)\n",
    "        return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command allows downloading the dataset directly\n",
    "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Otherwise, if the data have been downloaded before, it is possible to specify the path to the data files\n",
    "# and use the functions provided before\n",
    "#(x_train, y_train), (x_test, y_test)= load_CIFAR10_data('/home/josh/Documents/cs231n/cifar-10-batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "              'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (50000, 32, 32, 3)\n",
      "Train labels shape:  (50000, 1)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# It is a good idea to visualise the data we just loaded.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape:  (32, 32, 3)\n",
      "The image is a:  Truck\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, as expected we have 50,000 training images and 10,0000 for testing. What is\n",
    "# each image shape? Let's find out\n",
    "print('Train image shape: ', x_train[1].shape)\n",
    "\n",
    "# We can randomly show any of the numbers\n",
    "rnd_idx = np.random.randint(x_train.shape[0])\n",
    "plt.imshow(x_train[rnd_idx].astype(np.int32)) #Using astype guarantess imshow work\n",
    "print('The image is a: ', classes[y_train[rnd_idx,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and to use default TF settings, we cast the data to int32 and float32\n",
    "# Since, the dataset is relatively small, in most cases, this is not a problem\n",
    "y_train = y_train.astype(np.int32).reshape(-1)\n",
    "y_test = y_test.astype(np.int32).reshape(-1)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we define a function to divide our data in N mini batches, this is an important step, to allow for mini batch \n",
    "# gradient descent\n",
    "\n",
    "def mini_batches(mini_batch_size, data_x, data_y = None):\n",
    "    # First we validate the data is of the expected type, and the number of labels meet the number of training samples\n",
    "    assert data_x.shape[0] == data_y.shape[0], 'X number of samples not equal to Y number of samples'\n",
    "    assert (isinstance(data_x, np.ndarray) and isinstance(data_y, np.ndarray)), 'Data not numpy array'\n",
    "    \n",
    "    N = data_x.shape[0] # Get the number of samples\n",
    "    idxs = np.arange(N) \n",
    "    # Shuffle data, this may not be so critical in this example, but it is important for most applications, to avoid\n",
    "    # strong correlations in mini batches\n",
    "    np.random.shuffle(idxs)\n",
    "    data_x = data_x[idxs] # Shuffle training samples\n",
    "    data_y = data_y[idxs] # Shuffle labels (don't forget)\n",
    "    \n",
    "    # Finally return the data in minibatches of the desired size\n",
    "    # List comprehension is so cool, technically this is returning a generator but the principle is the same\n",
    "    return ((data_x[i:i+mini_batch_size], data_y[i:i+mini_batch_size]) for i in range(0, N, mini_batch_size))\n",
    "\n",
    "type(mini_batches(64, x_test, y_test)) # Check type returned by function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Low Level CNN \n",
    "In this part of the notebook, we will implement a complete CNN using Low Level TF only. I.e., we will not use those nice Keras layers. We will use 2D convolutions, with defined weights, which will be updated with tf.gradients (I mean, not that low level after all). \n",
    "For this notebokk, we will follow a simple approach where all the filters are 3x3 in size.\n",
    "We will use 5-layer CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_five_layer_CNN(num_filters):\n",
    "    '''\n",
    "    This function will be used to initialise all the learnable parameters. For simplicity all the filters\n",
    "    are size 3x3.\n",
    "    \n",
    "    Inputs:\n",
    "    - num_filters: List containing the number of filters for each layer, including the input (e.g. 3 RGB channels)\n",
    "      This will have the shape [channels, k1, k2, k3, k4, classes]\n",
    "\n",
    "    Outputs:\n",
    "    - Parameters: Dictionary with network parameters, where element 'W1' and 'b1' represent weigth and bias\n",
    "      for layer one\n",
    "    '''\n",
    "    # Dictionary  to save  parameters\n",
    "    parameters = {}\n",
    "        \n",
    "    # Manually assing filter size \n",
    "    fsize = 3 \n",
    "    \n",
    "    # Number of filters\n",
    "    c, k1, k2, k3, k4, classes = num_filters\n",
    "\n",
    "    # For this architecture, we will use the more robust initialization described in:   \n",
    "    #     He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance \n",
    "    #     on ImageNet Classification, ICCV 2015, https://arxiv.org/abs/1502.01852\n",
    "    \n",
    "    parameters['W1'] = tf.Variable(tf.random_normal((fsize, fsize, c, k1), \n",
    "                          dtype=tf.float32) * np.sqrt(2.0 / ((32*32*c))),\n",
    "                          dtype=tf.float32, name='W1')\n",
    "    parameters['b1'] = tf.Variable(tf.zeros(k1, dtype=tf.float32), dtype=tf.float32, name='conv_b1')\n",
    "    \n",
    "    parameters['W2'] = tf.Variable(tf.random_normal((fsize, fsize, k1, k2),\n",
    "                          dtype=tf.float32) * np.sqrt(2.0 / (32*32*k1)),\n",
    "                          dtype=tf.float32, name='conv_w2')\n",
    "    parameters['b2'] = tf.Variable(tf.zeros(k2, dtype=tf.float32), dtype=tf.float32, name='conv_b2')\n",
    "    \n",
    "    parameters['W3'] = tf.Variable(tf.random_normal((fsize, fsize, k2, k3), \n",
    "                          dtype=tf.float32) * np.sqrt(2.0 / (16*16*k2)),\n",
    "                          dtype=tf.float32, name='conv_w3')\n",
    "    parameters['b3'] = tf.Variable(tf.zeros(k3, dtype=tf.float32), dtype=tf.float32, name='conv_b3')\n",
    "    \n",
    "    parameters['W4'] = tf.Variable(tf.random_normal((fsize, fsize, k3, k4), \n",
    "                          dtype=tf.float32) * np.sqrt(2.0 / (16*16*k3)),\n",
    "                          dtype=tf.float32, name='conv_w4')\n",
    "    parameters['b4'] = tf.Variable(tf.zeros(k4, dtype=tf.float32), dtype=tf.float32, name='conv_b4')\n",
    "    \n",
    "    \n",
    "    # If FC\n",
    "    parameters['W5'] = tf.Variable(tf.random_normal((8*8*k4, classes), \n",
    "                          dtype=tf.float32) * np.sqrt(2.0 / (8*8*k4)),\n",
    "                          dtype=tf.float32, name='W5')\n",
    "\n",
    "                                               \n",
    "    parameters['b5'] = tf.Variable(tf.zeros((classes), dtype=tf.float32), dtype=tf.float32, name='b5')\n",
    "\n",
    "    \n",
    "    return parameters, classes\n",
    "\n",
    "def five_layer_CNN(x, params, classes, training=True, dropout_p=0.5):\n",
    "    '''\n",
    "    Create the inference graph, define the network architecture.\n",
    "    Notice that Tensorflow data format is of the shape N x H x W x C\n",
    "    \n",
    "    Inputs:\n",
    "    - x: Tensor with training or test images of shape (N, 32, 32, 3) for CIFAR10\n",
    "    - params: Tuple with all the learnable weights and biases\n",
    "    - classes: number of classes, i.e. the neturons in the output layer\n",
    "    '''\n",
    "     \n",
    "    \n",
    "    # We use conv2d function to apply the filter, we will use 'SAME' convolutions so that the size is keep \n",
    "    # constant, depending on max pooling for downsampling. \n",
    "    \n",
    "    \n",
    "    # First layer\n",
    "    # The input shape is (N, H, W, C), filter shape is (fsize, fsize, C, K1)\n",
    "    # The output size after the convolution is then, (N, H, W, k1)\n",
    "    conv1_out = tf.nn.conv2d(input=x,\n",
    "                             filter=params['W1'], \n",
    "                             strides=[1,1,1,1],\n",
    "                             padding='SAME',\n",
    "                             name='conv1_out')\n",
    "    conv1_out += params['b1'][None, None, None, :] # Add bias, axes are added (reshape) to broadcast correctly\n",
    "    conv1_out_relu = tf.nn.relu(conv1_out, name='conv1_out_relu')\n",
    "    if training: conv1_out_relu = tf.nn.dropout(x=conv1_out_relu, keep_prob=dropout_p)\n",
    "    \n",
    "    # Second layer\n",
    "    # Input shape = (N, H, W, k1), filter shape = (fsize, fsize, k1, k2)\n",
    "    # Ouput shape = (N, H, W, k2)\n",
    "    conv2_out = tf.nn.conv2d(input=conv1_out_relu,\n",
    "                             filter=params['W2'],\n",
    "                             strides=[1,1,1,1],\n",
    "                             padding='SAME',\n",
    "                             name='conv2_out')\n",
    "    conv2_out += params['b2'][None, None, None, :] # Add bias, axes are added (reshape) to broadcast correctly\n",
    "    conv2_out_relu = tf.nn.relu(conv2_out, name='conv2_out_relu')\n",
    "    if training: conv2_out_relu = tf.nn.dropout(x=conv2_out_relu, keep_prob=dropout_p)\n",
    "    max_pooled_layer2 = tf.nn.max_pool(value=conv2_out_relu,\n",
    "                                       ksize=[1,2,2,1],\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       padding='VALID',\n",
    "                                       data_format='NHWC',\n",
    "                                       name='max_pooled_layer2')\n",
    "   \n",
    "    # Third layer\n",
    "    # Input shape = (N, 16, 16, k2), filter shape = (fsize, fsize, k2, k3)\n",
    "    # Ouput shape = (N, 16, 16, k3)\n",
    "    conv3_out = tf.nn.conv2d(input=max_pooled_layer2,\n",
    "                             filter=params['W3'],\n",
    "                             strides=[1,1,1,1],\n",
    "                             padding='SAME',\n",
    "                             name='conv3_out')\n",
    "    conv3_out += params['b3'][None, None, None, :] # Add bias, axes are added (reshape) to broadcast correctly\n",
    "    conv3_out_relu = tf.nn.relu(conv3_out, name='conv3_out_relu')\n",
    "    if training: conv3_out_relu = tf.nn.dropout(x=conv3_out_relu, keep_prob=dropout_p)\n",
    "                          \n",
    "    # Fourth layer\n",
    "    # Input shape = (N, 16, 16, k3), filter shape = (fsize, fsize, k3, k4)\n",
    "    # Ouput shape = (N, 16, 16, k4)\n",
    "    conv4_out = tf.nn.conv2d(input=conv3_out_relu,\n",
    "                             filter=params['W4'],\n",
    "                             strides=[1,1,1,1],\n",
    "                             padding='SAME',\n",
    "                             name='conv4_out')\n",
    "    conv4_out += params['b4'][None, None, None, :] # Add bias, axes are added (reshape) to broadcast correctly\n",
    "    conv4_out_relu = tf.nn.relu(conv4_out, name='conv4_out_relu')\n",
    "    \n",
    "    if training: conv4_out_relu = tf.nn.dropout(x=conv4_out_relu, keep_prob=dropout_p)\n",
    "    \n",
    "    max_pooled_layer4 = tf.nn.max_pool(value=conv4_out_relu,\n",
    "                                       ksize=[1,2,2,1],\n",
    "                                       strides=[1,2,2,1],\n",
    "                                       padding='VALID',\n",
    "                                       data_format='NHWC',\n",
    "                                       name='max_pooled_layer4')\n",
    "                          \n",
    "    # Fifth layer\n",
    "    # Input shape = (N, 8, 8, k4), filter shape = (8 x 8 k4, classes)\n",
    "    # Ouput shape = (N, classes)\n",
    "    scores = tf.layers.flatten(max_pooled_layer4)\n",
    "    scores = tf.matmul(scores, params['W5']) + params['b5']\n",
    "        \n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, training our model we will use a simple test function to check if the output shape tensor matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10)\n"
     ]
    }
   ],
   "source": [
    "# Help function to test if the inference graph produces the expected output dimensions given an input\n",
    "# in this case the output should be of shape (N, 10)\n",
    "\n",
    "def test_vanilla_CNN(num_samples):\n",
    "    # Let us declare some useful constants\n",
    "    num_filters = [3, 16, 32, 32, 64, 10]\n",
    "    \n",
    "    # Reset the default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    \n",
    "    # Obtain parameters\n",
    "    parameters, classes = init_five_layer_CNN(num_filters)\n",
    "    \n",
    "    # Add scores to the graph\n",
    "    scores = five_layer_CNN(x, parameters, classes)\n",
    "    \n",
    "    # Create session and run it\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        test = sess.run(scores, feed_dict={x:x_train[:num_samples]})\n",
    "        print(test.shape)\n",
    "\n",
    "# Test our model output size\n",
    "test_vanilla_CNN(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the training model\n",
    "Let the fun begin! In this section we will build the trainin model.\n",
    "We will use a Tensorflow session to run the inference graph and the training operations. This will look similiar to test_vanilla_CNN(), plus the required components to define a loss function and carry out learning step operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a function that will define the graph\n",
    "def train_CNN(num_filters,\n",
    "              num_epochs=10,\n",
    "              learning_rate=0.05,\n",
    "              reg=0.0,\n",
    "              print_every=100,\n",
    "              minibatch_size = 64,\n",
    "              dropout_p=0.5):\n",
    "    '''\n",
    "    This function will be used to run our training graph by creating a Tensorflow session.\n",
    "    \n",
    "    Inputs:\n",
    "    - num_filters: List containing the number of filters for each layer, including the input (e.g. 3 RGB channels)\n",
    "      This will usually have the shape [channels, k1, k2, k3, k4, classes]\n",
    "    - num_epochs: Integer with the number of epochs to run, an epoch is a complete pass in the whole training set\n",
    "    - learning_rate: Float with the learning rate to use for updates, i.e. the step size towards the minimum\n",
    "    - reg: L2 regularization strength, default is set to 0 for no regularization\n",
    "    - print_every: This is a helping variable to stop during training and evaluate loss functions and accuracy\n",
    "    - minibatch_size: Integer with the number of elements in minibatch\n",
    "    \n",
    "    Outputs:\n",
    "    - updated_parameters: List with update parameters\n",
    "    '''\n",
    "    \n",
    "    # Firstly, let's reset default graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Then, we will define placeholdersfor the training data, we need to be careful with the shape\n",
    "    x = tf.placeholder(tf.float32, [None, 32, 32, 3], name = 'x_train') # Training data\n",
    "    y = tf.placeholder(tf.int32, [None, ], name = 'y_train') # Training data\n",
    "    \n",
    "    # Let us add the nodes to the stored graphs, \n",
    "    tf.add_to_collection('images', x)\n",
    "    tf.add_to_collection('labels', y)\n",
    "    \n",
    "    # Let us obtain the parameters we will use to run the graph and compute the score\n",
    "    parameters_dict, classes = init_five_layer_CNN(num_filters)\n",
    "    \n",
    "    # Add the scores\n",
    "    scores = five_layer_CNN(x, parameters_dict, classes, dropout_p=dropout_p)\n",
    "    \n",
    "    # Add forward pass node\n",
    "    scores_test = five_layer_CNN(x, parameters_dict, classes,training=False)\n",
    "    \n",
    "    # Before moving on, save the scores so that we can run the graph from a restore\n",
    "    tf.add_to_collection('scores', scores)\n",
    "    \n",
    "    # Once the scores have been computed, we need to calculate our loss function (or error function)\n",
    "    # This will allow us to estimate how far we are from the true value\n",
    "#     losses = tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(labels=y, logits=scores)\n",
    "    \n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = scores, labels=y)\n",
    "    data_loss = tf.reduce_mean(losses, name='data_loss') # In case we use regularisation on our weights\n",
    "    \n",
    "    # Add L2 regularisation, this will be a bit tedious\n",
    "    reg_loss = tf.reduce_mean(reg * (tf.nn.l2_loss(parameters_dict['W1']) #Check if reducing mean is needed\n",
    "               + tf.nn.l2_loss(parameters_dict['W2'])\n",
    "               + tf.nn.l2_loss(parameters_dict['W3'])\n",
    "               + tf.nn.l2_loss(parameters_dict['W4']) \n",
    "               + tf.nn.l2_loss(parameters_dict['W5'])))\n",
    "    \n",
    "    # Calculate total loss\n",
    "    loss = data_loss + reg_loss\n",
    "                 \n",
    "    # Now we need to use Tensorflow magic to calculate the gradients, remember that is still a bit of Low level\n",
    "    # tensorflow\n",
    "    # first, we need to convert the parameters dictionary to list\n",
    "    parameters = [(param) for (key, param) in parameters_dict.items()]\n",
    "        \n",
    "    parameter_gradients = tf.gradients(loss, parameters)\n",
    "\n",
    "    # We need to update the weights manually, for this we can use tf.assign, or tf.assing_sub\n",
    "    # Using SGD, we need to update each parameter independently, so list comprehension works well\n",
    "    updated_parameters = [tf.assign_sub(w, learning_rate * grad) \n",
    "                          for w, grad in zip(parameters, parameter_gradients)]\n",
    "    \n",
    "   \n",
    "    # Save gradients\n",
    "    tf.add_to_collection('weights', updated_parameters)\n",
    "    \n",
    "    # Create saver to save the model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Helping variables to save useful information\n",
    "    best_acc = 0.0\n",
    "    accuracies = np.zeros((int(x_train.shape[0]/minibatch_size/print_every)) + 1)\n",
    "    # Now, recall we have only created the graph, in order to run it and train, we need a Session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print('Starting epoch: ', epoch)\n",
    "            for iteration, (x_mb, y_mb) in enumerate(mini_batches(minibatch_size, x_train, y_train)):\n",
    "                # The following line runs the loss for the minibatch.\n",
    "                # Recall TF knows what is needed to run a function and will run accordingy, e.g. scores before the\n",
    "                # loss. Also note that we only need to feed the data needed into a placeholder. In this case, we \n",
    "                # feed the minibatch training samples and labels\n",
    "                loss_mb, update_param = sess.run([loss, updated_parameters], feed_dict={x:x_mb, y:y_mb})\n",
    "                \n",
    "                if iteration % print_every == 0:\n",
    "                    # We define this function in the next cell\n",
    "                    accuracy = compute_accuracy(sess, minibatch_size, scores_test, x)\n",
    "                    accuracies[int(iteration/100)]=accuracy # save current accuracy\n",
    "#                     losses[int(iteration/print_every)] = loss_mb\n",
    "                    if accuracy > best_acc:\n",
    "                        best_acc = accuracy\n",
    "                    print('Iteration: %d Loss: %f Accuracy: %f Learning rate: %f Regularization: %f'\n",
    "                          %(iteration, loss_mb, accuracy, learning_rate, reg))\n",
    "            \n",
    "            acc_range = np.max(accuracies) - np.min(accuracies)\n",
    "            print('Accuracies mean: %f Accuracies std: %f Range: %f' %(np.mean(accuracies), np.std(accuracies),\n",
    "                                                            acc_range))\n",
    "#             if (acc_range < 0.10):\n",
    "#                 learning_rate = 0.90 * learning_rate\n",
    "            \n",
    "        saver.save(sess, 'checkpoint_file')\n",
    "        print('Best accuracy: %f', best_acc)\n",
    "\n",
    "        \n",
    "    return update_param, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(sess, minibatch_size, scores, x):\n",
    "    '''\n",
    "    This function computes the accuracy of the current model\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: it needs a current tf.Session to run the scores\n",
    "    - minibatch_size: The size of the mini batch to run the scores\n",
    "    - scores: TF operation to run\n",
    "    - x: test data\n",
    "    \n",
    "    Outputs:\n",
    "    - acc: Accuracy\n",
    "    \n",
    "    '''\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    for it, (xtest_mb, ytest_mb) in enumerate(mini_batches(minibatch_size, x_test, y_test)):\n",
    "        scores_test = sess.run(scores, feed_dict={x:xtest_mb})\n",
    "        y_pred = np.argmax(scores_test, axis=1)\n",
    "\n",
    "#         In case we would like to compare some elements of the predicted and ground truth arrays\n",
    "#         if it % 200 == 0:\n",
    "#             print('y_pred: ', y_pred[:10])\n",
    "#             print('y_test: ', ytest_mb[:10])\n",
    "\n",
    "        num_samples += xtest_mb.shape[0]\n",
    "        num_correct += np.sum(np.equal(y_pred, ytest_mb))\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:  0\n",
      "Iteration: 0 Loss: 2.322904 Accuracy: 0.102600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 2.288781 Accuracy: 0.103900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 2.200064 Accuracy: 0.170400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 2.013342 Accuracy: 0.245500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.967280 Accuracy: 0.312600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 2.075110 Accuracy: 0.301900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.820794 Accuracy: 0.338400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.776026 Accuracy: 0.356000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.241413 Accuracies std: 0.096502 Range: 0.253400\n",
      "Starting epoch:  1\n",
      "Iteration: 0 Loss: 1.943289 Accuracy: 0.327700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.790961 Accuracy: 0.392400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.778990 Accuracy: 0.395000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.638136 Accuracy: 0.408000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.793595 Accuracy: 0.407000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.653294 Accuracy: 0.432400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.787415 Accuracy: 0.436200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.396333 Accuracy: 0.432400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.403887 Accuracies std: 0.033000 Range: 0.108500\n",
      "Starting epoch:  2\n",
      "Iteration: 0 Loss: 1.564600 Accuracy: 0.444000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.652814 Accuracy: 0.452300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.478381 Accuracy: 0.449100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.364909 Accuracy: 0.477200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.579311 Accuracy: 0.488100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.610121 Accuracy: 0.488800 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.425755 Accuracy: 0.485500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.310224 Accuracy: 0.481500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.470812 Accuracies std: 0.017770 Range: 0.044800\n",
      "Starting epoch:  3\n",
      "Iteration: 0 Loss: 1.337005 Accuracy: 0.481200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.421140 Accuracy: 0.493100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.430001 Accuracy: 0.515300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.176781 Accuracy: 0.517700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.241896 Accuracy: 0.520600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.372631 Accuracy: 0.521500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.416303 Accuracy: 0.521600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.429896 Accuracy: 0.509800 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.510100 Accuracies std: 0.014060 Range: 0.040400\n",
      "Starting epoch:  4\n",
      "Iteration: 0 Loss: 1.296214 Accuracy: 0.527900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.388539 Accuracy: 0.468100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.350938 Accuracy: 0.537000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.345744 Accuracy: 0.558500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.213816 Accuracy: 0.548000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.008577 Accuracy: 0.552800 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.125702 Accuracy: 0.546600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.186730 Accuracy: 0.537900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.534600 Accuracies std: 0.026716 Range: 0.090400\n",
      "Starting epoch:  5\n",
      "Iteration: 0 Loss: 1.327919 Accuracy: 0.566500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.111413 Accuracy: 0.555200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.421850 Accuracy: 0.569000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.318467 Accuracy: 0.557700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.230872 Accuracy: 0.587900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.337231 Accuracy: 0.585300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.352661 Accuracy: 0.579100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.190043 Accuracy: 0.583200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.572987 Accuracies std: 0.011852 Range: 0.032700\n",
      "Starting epoch:  6\n",
      "Iteration: 0 Loss: 1.086940 Accuracy: 0.576000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 1.170405 Accuracy: 0.577000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 0.837247 Accuracy: 0.593200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 0.922462 Accuracy: 0.584200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.151289 Accuracy: 0.602200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.126868 Accuracy: 0.606800 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 1.184461 Accuracy: 0.600900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.185959 Accuracy: 0.604000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.593037 Accuracies std: 0.011624 Range: 0.030800\n",
      "Starting epoch:  7\n",
      "Iteration: 0 Loss: 1.142164 Accuracy: 0.597500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 0.825199 Accuracy: 0.607300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.016455 Accuracy: 0.606100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.029817 Accuracy: 0.611700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 1.072990 Accuracy: 0.606000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.002415 Accuracy: 0.624300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 0.902662 Accuracy: 0.613700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 0.947234 Accuracy: 0.622700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.611163 Accuracies std: 0.008414 Range: 0.026800\n",
      "Starting epoch:  8\n",
      "Iteration: 0 Loss: 1.196538 Accuracy: 0.605200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 0.964938 Accuracy: 0.622300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 1.200382 Accuracy: 0.620500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 0.985689 Accuracy: 0.636000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 400 Loss: 0.937083 Accuracy: 0.640400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.194471 Accuracy: 0.632700 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 0.906335 Accuracy: 0.631200 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.061110 Accuracy: 0.640900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.628650 Accuracies std: 0.011301 Range: 0.035700\n",
      "Starting epoch:  9\n",
      "Iteration: 0 Loss: 1.015358 Accuracy: 0.637500 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 100 Loss: 0.762996 Accuracy: 0.643400 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 200 Loss: 0.885376 Accuracy: 0.638300 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 300 Loss: 1.061478 Accuracy: 0.645300 Learning rate: 0.001850 Regularization: 0.000337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 400 Loss: 0.815536 Accuracy: 0.637100 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 500 Loss: 1.258722 Accuracy: 0.645600 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 600 Loss: 0.936822 Accuracy: 0.650900 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Iteration: 700 Loss: 1.016031 Accuracy: 0.642000 Learning rate: 0.001850 Regularization: 0.000337\n",
      "Accuracies mean: 0.642512 Accuracies std: 0.004484 Range: 0.013800\n",
      "WARNING:tensorflow:Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "Best accuracy: %f 0.6509\n",
      "Accuracy with LR=0.001850 and Reg=0.000337 ---> 0.650900\n",
      "Starting epoch:  0\n",
      "Iteration: 0 Loss: 2.299997 Accuracy: 0.094900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 2.305932 Accuracy: 0.124500 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 2.299893 Accuracy: 0.132200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 2.302691 Accuracy: 0.142300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 2.290850 Accuracy: 0.148900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 2.303822 Accuracy: 0.150900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 2.289772 Accuracy: 0.164800 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 2.299227 Accuracy: 0.157600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.139513 Accuracies std: 0.020801 Range: 0.069900\n",
      "Starting epoch:  1\n",
      "Iteration: 0 Loss: 2.289827 Accuracy: 0.158200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 2.287152 Accuracy: 0.180700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 2.291822 Accuracy: 0.167200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 2.282834 Accuracy: 0.171300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 2.257902 Accuracy: 0.170500 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 2.246089 Accuracy: 0.194000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 2.232959 Accuracy: 0.192900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 2.240305 Accuracy: 0.201700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.179563 Accuracies std: 0.014306 Range: 0.043500\n",
      "Starting epoch:  2\n",
      "Iteration: 0 Loss: 2.218237 Accuracy: 0.205100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 2.153119 Accuracy: 0.211000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 2.202636 Accuracy: 0.236700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 2.189194 Accuracy: 0.254300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 2.197545 Accuracy: 0.249200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 2.091857 Accuracy: 0.248200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 2.063588 Accuracy: 0.264900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 2.048502 Accuracy: 0.273000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.242800 Accuracies std: 0.022563 Range: 0.067900\n",
      "Starting epoch:  3\n",
      "Iteration: 0 Loss: 2.023730 Accuracy: 0.276500 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 2.027982 Accuracy: 0.284000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.988350 Accuracy: 0.288600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.938593 Accuracy: 0.282200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.966417 Accuracy: 0.300900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 2.008769 Accuracy: 0.305800 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 2.113698 Accuracy: 0.301700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.770292 Accuracy: 0.315300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.294375 Accuracies std: 0.012616 Range: 0.038800\n",
      "Starting epoch:  4\n",
      "Iteration: 0 Loss: 1.981715 Accuracy: 0.306600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.918349 Accuracy: 0.318000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.916942 Accuracy: 0.314400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.848836 Accuracy: 0.339100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.968658 Accuracy: 0.324600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 2.039479 Accuracy: 0.342600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.856961 Accuracy: 0.315800 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.937160 Accuracy: 0.346600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.325963 Accuracies std: 0.013930 Range: 0.040000\n",
      "Starting epoch:  5\n",
      "Iteration: 0 Loss: 1.783414 Accuracy: 0.333700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.854859 Accuracy: 0.343600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.897918 Accuracy: 0.349300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.948214 Accuracy: 0.353400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.678201 Accuracy: 0.360700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 1.691847 Accuracy: 0.347100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.789083 Accuracy: 0.369900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.775277 Accuracy: 0.366700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.353050 Accuracies std: 0.011413 Range: 0.036200\n",
      "Starting epoch:  6\n",
      "Iteration: 0 Loss: 1.774719 Accuracy: 0.372800 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.966509 Accuracy: 0.377300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.667987 Accuracy: 0.385300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.704491 Accuracy: 0.383200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.726942 Accuracy: 0.395000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 1.914375 Accuracy: 0.382000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.607292 Accuracy: 0.392100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.581665 Accuracy: 0.396600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.385537 Accuracies std: 0.007930 Range: 0.023800\n",
      "Starting epoch:  7\n",
      "Iteration: 0 Loss: 1.593779 Accuracy: 0.383600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.719516 Accuracy: 0.386200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.558104 Accuracy: 0.401400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.747125 Accuracy: 0.391900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.881796 Accuracy: 0.407000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 1.797580 Accuracy: 0.414400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.677521 Accuracy: 0.406000 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.735432 Accuracy: 0.419400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.401237 Accuracies std: 0.012163 Range: 0.035800\n",
      "Starting epoch:  8\n",
      "Iteration: 0 Loss: 1.812887 Accuracy: 0.412100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.783596 Accuracy: 0.423400 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.693961 Accuracy: 0.414700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.741928 Accuracy: 0.419700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 1.699309 Accuracy: 0.424900 Learning rate: 0.000167 Regularization: 0.000016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 Loss: 1.460048 Accuracy: 0.425700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.616174 Accuracy: 0.431200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.828131 Accuracy: 0.434200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.423238 Accuracies std: 0.007088 Range: 0.022100\n",
      "Starting epoch:  9\n",
      "Iteration: 0 Loss: 1.531688 Accuracy: 0.430200 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 100 Loss: 1.511013 Accuracy: 0.426700 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 200 Loss: 1.777067 Accuracy: 0.438600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 300 Loss: 1.473734 Accuracy: 0.432300 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 400 Loss: 2.023827 Accuracy: 0.422600 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 500 Loss: 1.564401 Accuracy: 0.439900 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 600 Loss: 1.640178 Accuracy: 0.449100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Iteration: 700 Loss: 1.618939 Accuracy: 0.436100 Learning rate: 0.000167 Regularization: 0.000016\n",
      "Accuracies mean: 0.434438 Accuracies std: 0.007806 Range: 0.026500\n",
      "WARNING:tensorflow:Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "Best accuracy: %f 0.4491\n",
      "Accuracy with LR=0.000167 and Reg=0.000016 ---> 0.449100\n",
      "Starting epoch:  0\n",
      "Iteration: 0 Loss: 2.306640 Accuracy: 0.122900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 2.284416 Accuracy: 0.109300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 2.300434 Accuracy: 0.100700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 2.285517 Accuracy: 0.156600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 2.242794 Accuracy: 0.157200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 2.300794 Accuracy: 0.113000 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 2.199197 Accuracy: 0.161800 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 2.309604 Accuracy: 0.099700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.127650 Accuracies std: 0.024897 Range: 0.062100\n",
      "Starting epoch:  1\n",
      "Iteration: 0 Loss: 2.569053 Accuracy: 0.100100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 2.287142 Accuracy: 0.163300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 2.102350 Accuracy: 0.294900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 2.230669 Accuracy: 0.174400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 2.076817 Accuracy: 0.232200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.904759 Accuracy: 0.275600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.984099 Accuracy: 0.312700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 2.004176 Accuracy: 0.221300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.221812 Accuracies std: 0.068089 Range: 0.212600\n",
      "Starting epoch:  2\n",
      "Iteration: 0 Loss: 1.832154 Accuracy: 0.312400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 1.995705 Accuracy: 0.295200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.684136 Accuracy: 0.397800 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.566125 Accuracy: 0.420800 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 1.820942 Accuracy: 0.381500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.783294 Accuracy: 0.372900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.622572 Accuracy: 0.459600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.577299 Accuracy: 0.475300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.389437 Accuracies std: 0.059687 Range: 0.180100\n",
      "Starting epoch:  3\n",
      "Iteration: 0 Loss: 1.990394 Accuracy: 0.415500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 1.451143 Accuracy: 0.487900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.649346 Accuracy: 0.509900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.489086 Accuracy: 0.523600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 1.239798 Accuracy: 0.533400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.279059 Accuracy: 0.535500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.187861 Accuracy: 0.534500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.254576 Accuracy: 0.479600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.502487 Accuracies std: 0.038527 Range: 0.120000\n",
      "Starting epoch:  4\n",
      "Iteration: 0 Loss: 1.409666 Accuracy: 0.535600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 1.411558 Accuracy: 0.563200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.152530 Accuracy: 0.569400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.081301 Accuracy: 0.571100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 1.566246 Accuracy: 0.548600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.022493 Accuracy: 0.585600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.017933 Accuracy: 0.584900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.252517 Accuracy: 0.609100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.570937 Accuracies std: 0.021437 Range: 0.073500\n",
      "Starting epoch:  5\n",
      "Iteration: 0 Loss: 2.181809 Accuracy: 0.345900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 1.308441 Accuracy: 0.587200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.251286 Accuracy: 0.589900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.402253 Accuracy: 0.570700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 0.949552 Accuracy: 0.571200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.186342 Accuracy: 0.604000 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.290449 Accuracy: 0.618500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.099676 Accuracy: 0.603900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.561412 Accuracies std: 0.082902 Range: 0.272600\n",
      "Starting epoch:  6\n",
      "Iteration: 0 Loss: 1.564335 Accuracy: 0.553200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 1.116819 Accuracy: 0.595000 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.030722 Accuracy: 0.627400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.261085 Accuracy: 0.633600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 0.962229 Accuracy: 0.618700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.100017 Accuracy: 0.606700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 0.997508 Accuracy: 0.613300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 0.943304 Accuracy: 0.636500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.610550 Accuracies std: 0.025269 Range: 0.083300\n",
      "Starting epoch:  7\n",
      "Iteration: 0 Loss: 1.190676 Accuracy: 0.616200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 0.712537 Accuracy: 0.596600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 0.903126 Accuracy: 0.606000 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 0.620488 Accuracy: 0.637500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 0.894635 Accuracy: 0.605500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.134658 Accuracy: 0.552500 Learning rate: 0.041342 Regularization: 0.000017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600 Loss: 0.963944 Accuracy: 0.633300 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 0.794537 Accuracy: 0.620800 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.608550 Accuracies std: 0.024898 Range: 0.085000\n",
      "Starting epoch:  8\n",
      "Iteration: 0 Loss: 0.921699 Accuracy: 0.624900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 0.999224 Accuracy: 0.610100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 1.043718 Accuracy: 0.635100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 0.602070 Accuracy: 0.644600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 0.708678 Accuracy: 0.615400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 1.419067 Accuracy: 0.618500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 0.797740 Accuracy: 0.623600 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.087325 Accuracy: 0.636700 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.626113 Accuracies std: 0.011020 Range: 0.034500\n",
      "Starting epoch:  9\n",
      "Iteration: 0 Loss: 1.089758 Accuracy: 0.574100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 100 Loss: 0.773293 Accuracy: 0.628900 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 200 Loss: 0.828452 Accuracy: 0.649100 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 300 Loss: 1.029178 Accuracy: 0.632200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 400 Loss: 0.772370 Accuracy: 0.628000 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 500 Loss: 0.981980 Accuracy: 0.650500 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 600 Loss: 1.023904 Accuracy: 0.576400 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Iteration: 700 Loss: 1.004197 Accuracy: 0.639200 Learning rate: 0.041342 Regularization: 0.000017\n",
      "Accuracies mean: 0.622300 Accuracies std: 0.028291 Range: 0.076400\n",
      "WARNING:tensorflow:Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "Best accuracy: %f 0.6505\n",
      "Accuracy with LR=0.041342 and Reg=0.000017 ---> 0.650500\n",
      "Starting epoch:  0\n",
      "Iteration: 0 Loss: 2.296114 Accuracy: 0.099900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.301534 Accuracy: 0.100000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 2.303543 Accuracy: 0.102900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.297824 Accuracy: 0.123500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 2.294554 Accuracy: 0.147600 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 2.291607 Accuracy: 0.154700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 2.292660 Accuracy: 0.151500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 2.301323 Accuracy: 0.145100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.128150 Accuracies std: 0.022818 Range: 0.054800\n",
      "Starting epoch:  1\n",
      "Iteration: 0 Loss: 2.302736 Accuracy: 0.131700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.298227 Accuracy: 0.131300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 2.287792 Accuracy: 0.135400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.287894 Accuracy: 0.129700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 2.274548 Accuracy: 0.123100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 2.261698 Accuracy: 0.133300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 2.268849 Accuracy: 0.133300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 2.290320 Accuracy: 0.145600 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.132925 Accuracies std: 0.005896 Range: 0.022500\n",
      "Starting epoch:  2\n",
      "Iteration: 0 Loss: 2.261426 Accuracy: 0.130700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.300900 Accuracy: 0.142300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 2.265183 Accuracy: 0.142900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.274117 Accuracy: 0.151900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 2.268662 Accuracy: 0.161000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 2.266291 Accuracy: 0.214200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 2.234883 Accuracy: 0.197700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 2.248757 Accuracy: 0.193300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.166750 Accuracies std: 0.028797 Range: 0.083500\n",
      "Starting epoch:  3\n",
      "Iteration: 0 Loss: 2.207380 Accuracy: 0.205500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.189391 Accuracy: 0.222100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 2.229295 Accuracy: 0.241000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.169606 Accuracy: 0.250700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 2.133174 Accuracy: 0.254400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 2.122659 Accuracy: 0.266700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 2.198704 Accuracy: 0.262100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 2.136419 Accuracy: 0.271000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.246688 Accuracies std: 0.021325 Range: 0.065500\n",
      "Starting epoch:  4\n",
      "Iteration: 0 Loss: 2.023855 Accuracy: 0.278300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.141807 Accuracy: 0.280300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 1.990947 Accuracy: 0.276100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.079006 Accuracy: 0.289100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.872145 Accuracy: 0.298100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 2.029840 Accuracy: 0.293400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 1.972429 Accuracy: 0.301500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 1.900617 Accuracy: 0.306400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.290400 Accuracies std: 0.010619 Range: 0.030300\n",
      "Starting epoch:  5\n",
      "Iteration: 0 Loss: 2.164015 Accuracy: 0.307600 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 2.136696 Accuracy: 0.317600 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 2.075459 Accuracy: 0.312600 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 2.057159 Accuracy: 0.317300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.876916 Accuracy: 0.323500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 1.838014 Accuracy: 0.326000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 1.952048 Accuracy: 0.332400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 1.873132 Accuracy: 0.327300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.320538 Accuracies std: 0.007702 Range: 0.024800\n",
      "Starting epoch:  6\n",
      "Iteration: 0 Loss: 1.896662 Accuracy: 0.332200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 1.945521 Accuracy: 0.331800 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 1.843278 Accuracy: 0.337300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 1.905209 Accuracy: 0.342400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.753176 Accuracy: 0.332200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 1.866578 Accuracy: 0.346200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 2.023007 Accuracy: 0.347100 Learning rate: 0.000133 Regularization: 0.000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 700 Loss: 1.886645 Accuracy: 0.356900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.340762 Accuracies std: 0.008454 Range: 0.025100\n",
      "Starting epoch:  7\n",
      "Iteration: 0 Loss: 1.969300 Accuracy: 0.349200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 1.920740 Accuracy: 0.356200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 1.928621 Accuracy: 0.357800 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 1.675707 Accuracy: 0.359200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.715910 Accuracy: 0.364400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 1.957980 Accuracy: 0.361900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 1.898578 Accuracy: 0.361400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 1.931885 Accuracy: 0.370800 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.360113 Accuracies std: 0.005895 Range: 0.021600\n",
      "Starting epoch:  8\n",
      "Iteration: 0 Loss: 1.918167 Accuracy: 0.346400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 1.713098 Accuracy: 0.371500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 1.749054 Accuracy: 0.374100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 1.680110 Accuracy: 0.375900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.679765 Accuracy: 0.370900 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 1.802205 Accuracy: 0.354500 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 1.781582 Accuracy: 0.372200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 1.710683 Accuracy: 0.379700 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.368150 Accuracies std: 0.010744 Range: 0.033300\n",
      "Starting epoch:  9\n",
      "Iteration: 0 Loss: 2.030017 Accuracy: 0.382800 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 100 Loss: 1.797912 Accuracy: 0.390100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 200 Loss: 1.859350 Accuracy: 0.373800 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 300 Loss: 1.860935 Accuracy: 0.386400 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 400 Loss: 1.728629 Accuracy: 0.390100 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 500 Loss: 1.695790 Accuracy: 0.397200 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 600 Loss: 1.628343 Accuracy: 0.375300 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Iteration: 700 Loss: 1.753740 Accuracy: 0.380000 Learning rate: 0.000133 Regularization: 0.000006\n",
      "Accuracies mean: 0.384463 Accuracies std: 0.007499 Range: 0.023400\n",
      "WARNING:tensorflow:Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "Best accuracy: %f 0.3972\n",
      "Accuracy with LR=0.000133 and Reg=0.000006 ---> 0.397200\n",
      "Starting epoch:  0\n",
      "Iteration: 0 Loss: 2.338912 Accuracy: 0.100300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 2.236847 Accuracy: 0.113000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 2.285868 Accuracy: 0.184500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 2.322768 Accuracy: 0.156500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 2.090405 Accuracy: 0.223300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 2.138666 Accuracy: 0.244700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 1.878802 Accuracy: 0.312800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 1.733386 Accuracy: 0.363000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.212262 Accuracies std: 0.086799 Range: 0.262700\n",
      "Starting epoch:  1\n",
      "Iteration: 0 Loss: 3.232823 Accuracy: 0.295300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 1.495888 Accuracy: 0.431600 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 1.584724 Accuracy: 0.436700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 1.594114 Accuracy: 0.454600 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 1.596842 Accuracy: 0.511300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 1.329799 Accuracy: 0.538800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 1.635225 Accuracy: 0.497200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 1.365817 Accuracy: 0.553200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.464838 Accuracies std: 0.076835 Range: 0.257900\n",
      "Starting epoch:  2\n",
      "Iteration: 0 Loss: 1.876086 Accuracy: 0.471900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 1.057965 Accuracy: 0.595500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 1.183702 Accuracy: 0.531100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 1.208819 Accuracy: 0.560000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 1.101484 Accuracy: 0.632200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 1.026222 Accuracy: 0.599100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 1.051619 Accuracy: 0.638200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 1.227000 Accuracy: 0.634700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.582838 Accuracies std: 0.054911 Range: 0.166300\n",
      "Starting epoch:  3\n",
      "Iteration: 0 Loss: 1.258949 Accuracy: 0.618200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.988079 Accuracy: 0.650000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.933239 Accuracy: 0.655800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.859172 Accuracy: 0.648900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 1.101842 Accuracy: 0.661500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.893753 Accuracy: 0.674500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.824366 Accuracy: 0.672500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.759536 Accuracy: 0.686300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.658462 Accuracies std: 0.019460 Range: 0.068100\n",
      "Starting epoch:  4\n",
      "Iteration: 0 Loss: 1.560369 Accuracy: 0.603700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.868171 Accuracy: 0.680400 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 1.275885 Accuracy: 0.634400 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 1.164214 Accuracy: 0.672500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.533147 Accuracy: 0.692900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.889007 Accuracy: 0.685400 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.843508 Accuracy: 0.703200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.857108 Accuracy: 0.701500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.671750 Accuracies std: 0.032804 Range: 0.099500\n",
      "Starting epoch:  5\n",
      "Iteration: 0 Loss: 1.116275 Accuracy: 0.672500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.745915 Accuracy: 0.714300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.599669 Accuracy: 0.687100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.625892 Accuracy: 0.718700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.931606 Accuracy: 0.688000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.798992 Accuracy: 0.697200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.756925 Accuracy: 0.717100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.698088 Accuracy: 0.717100 Learning rate: 0.014032 Regularization: 0.001462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies mean: 0.701500 Accuracies std: 0.016566 Range: 0.046200\n",
      "Starting epoch:  6\n",
      "Iteration: 0 Loss: 0.620006 Accuracy: 0.702700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.436950 Accuracy: 0.725800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.473519 Accuracy: 0.709900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.651775 Accuracy: 0.718600 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.805248 Accuracy: 0.714300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.649656 Accuracy: 0.721100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.751462 Accuracy: 0.716700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.612254 Accuracy: 0.726700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.716975 Accuracies std: 0.007526 Range: 0.024000\n",
      "Starting epoch:  7\n",
      "Iteration: 0 Loss: 0.604606 Accuracy: 0.708100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.538696 Accuracy: 0.714700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.599023 Accuracy: 0.711300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.911424 Accuracy: 0.719300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.382325 Accuracy: 0.719500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.552759 Accuracy: 0.704300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.485539 Accuracy: 0.712200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.696499 Accuracy: 0.718200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.713450 Accuracies std: 0.005166 Range: 0.015200\n",
      "Starting epoch:  8\n",
      "Iteration: 0 Loss: 0.774279 Accuracy: 0.712100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.335538 Accuracy: 0.727300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.544336 Accuracy: 0.725300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.499107 Accuracy: 0.731000 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.437705 Accuracy: 0.716900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.404700 Accuracy: 0.721300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.776789 Accuracy: 0.712100 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.911051 Accuracy: 0.729900 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.721988 Accuracies std: 0.007114 Range: 0.018900\n",
      "Starting epoch:  9\n",
      "Iteration: 0 Loss: 0.612706 Accuracy: 0.725700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 100 Loss: 0.568398 Accuracy: 0.729800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 200 Loss: 0.377165 Accuracy: 0.730800 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 300 Loss: 0.505058 Accuracy: 0.710300 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 400 Loss: 0.557986 Accuracy: 0.714500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 500 Loss: 0.471203 Accuracy: 0.720200 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 600 Loss: 0.843930 Accuracy: 0.691500 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Iteration: 700 Loss: 0.590330 Accuracy: 0.725700 Learning rate: 0.014032 Regularization: 0.001462\n",
      "Accuracies mean: 0.718562 Accuracies std: 0.012236 Range: 0.039300\n",
      "WARNING:tensorflow:Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "Best accuracy: %f 0.731\n",
      "Accuracy with LR=0.014032 and Reg=0.001462 ---> 0.731000\n",
      "Best acc after 10 epochs: 0.731000 with lr: 1.403179e-02 and reg: 0.001462\n"
     ]
    }
   ],
   "source": [
    "# Let us train! We should expect achieving better perfomance than that we achieved with the fully connected\n",
    "# network in previous experiment. Here, we will experiment with hyperparameter selection\n",
    "num_filters = [3, 32, 64, 64, 128, 10]\n",
    "\n",
    "#10e-3 if random init with std = 0.01\n",
    "# Let us try for 10 combinations of regularization and learning rates\n",
    "total_tests = 5\n",
    "num_epochs = 10\n",
    "accuracies = {}\n",
    "for test in range(total_tests):\n",
    "    learning_rate = 10**np.random.uniform(-4, -1)\n",
    "    reg = 10**np.random.uniform(-5.5, -2.5)\n",
    "    update_param, best_acc = train_CNN(\n",
    "                num_filters,\n",
    "                num_epochs=num_epochs,\n",
    "                learning_rate=learning_rate, \n",
    "                reg=reg,\n",
    "                print_every=100,\n",
    "                minibatch_size = 64, \n",
    "                dropout_p=1.0)\n",
    "    accuracies[(learning_rate, reg)] = best_acc\n",
    "    print('Accuracy with LR=%f and Reg=%f ---> %f' %(learning_rate, reg, best_acc))\n",
    "\n",
    "accs = sorted([(acc, lr_reg )for lr_reg, acc in accuracies.items()], reverse=1)\n",
    "print('Best acc after %d epochs: %f with lr: %e and reg: %f' \n",
    "      %(num_epochs, accs[0][0], accs[0][1][0], accs[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From the previous result, we see that the best accuracy is with LR = 1.679e-02, thus we can look in a finer\n",
    "# interval, e.g. 10**[-2, -1.5]\n",
    "# since Reg = 0.000005, let us look in the interval [-5.5, -4.5]\n",
    "# Then in this step, we will look in narrower interval to fine tune the LR and Reg values.\n",
    "\n",
    "# Let us try for 10 combinations of regularization and learning rates\n",
    "total_tests = 5\n",
    "num_epochs = 10\n",
    "accuracies = {}\n",
    "for test in range(total_tests):\n",
    "    learning_rate = 10**np.random.uniform(-2.5, -1.5)\n",
    "    reg = 10**np.random.uniform(-5, -4)\n",
    "    update_param, best_acc = train_CNN(\n",
    "                num_filters,\n",
    "                num_epochs=num_epochs,\n",
    "                learning_rate=learning_rate, \n",
    "                reg=reg,\n",
    "                print_every=100,\n",
    "                minibatch_size = 64, \n",
    "                dropout_p=1.0)\n",
    "    accuracies[(learning_rate, reg)] = best_acc\n",
    "    print('Accuracy with LR=%f and Reg=%f ---> %f' %(learning_rate, reg, best_acc))\n",
    "\n",
    "accs = sorted([(acc, lr_reg )for lr_reg, acc in accuracies.items()], reverse=1)\n",
    "print('Best acc after %d epochs: %f with lr: %e and reg: %f' \n",
    "      %(num_epochs, accs[0][0], accs[0][1][0], accs[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is:  Frog\n",
      "The correct class is:  Frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAF19JREFUeJzt3XuMXNV9B/Dvb2Zn32vjtVl7/cDGjgEbAwaMSwolhLTIoLRA1CCoWlkq6qZVkIKU/IGoVGj/ClUhjdSIyilWTER5lEdAiPKoSwqpFIIxfjvGD4xZe9n1Y9fe987j1z/mulmb8zs7vjtzx8v5fqTV7t6zZ+7ZO/ObO3N/c35HVBVEFJ5UtQdARNXB4CcKFIOfKFAMfqJAMfiJAsXgJwoUg58oUAx+okAx+IkCVTOZziKyBsCPAaQB/Juq/tD/9ymFGLtMZTwdreeogt2nkPe05ew2ePoRTQGqKqX8ncT9eK+IpAF8DOCPAHQC+ADAvaq6y+yTqlWpme1sSzfPMfeltQ3uhvyoPcChPrOpMHrU7lc4abep+8km2A9Iex5iZlMFDlZJj/Sz+IahSMccSMx/zozBeLdXavBP5mX/agD7VPWAqo4BeBbAHZO4PSJK0GSCfx6Az8b93hltI6IpYDLv+V0vLb7wOkVEOgB0FH+L+XKKiMpuMsHfCWDBuN/nAzhy9h+p6joA64Die/5J7I+IymgyL/s/ALBURC4WkVoA9wB4tTzDIqJKi33mV9WciNwP4E0UX8+vV9Wd3k6pDNJN7c6mTE2j2S07MuDcnhv6wguNcY321f6UJ52XipH98CUOpzrfZWPfRWWzKRXn2jwgnqxuPPb9LGm7TdXz1rXge+z4UsjVeUE8qTy/qr4O4PUyjYWIEsRP+BEFisFPFCgGP1GgGPxEgWLwEwVqUlf7z5nmUSiccDaNDfWY3fKjRtouP2j2EY03O8+fdLHSVF/izy5Jnd2kvnOHcfyNyVEAoJ7j6NuV9+hbjZ6UY1rsx44vE6z+xGiMlsrimZ8oUAx+okAx+IkCxeAnChSDnyhQsct4xdqZpFTS7qvH3nGIUa7LN/RCvAkkE0xlOcftU4T3UNlX+33/dg2yzu0tdfbOajN2Wy7WJX2YpbXE02f+bHuS2Yle9/8FAF0nxsw23+Svcj96kijjRURTGIOfKFAMfqJAMfiJAsXgJwoUg58oUMlO7IEilXenSgqefJNKnLSd53kt7soqFm9mxff8GnccZS5o5xlGOm2vitRoLKQEAMsvdqfLli9pMvtkYKfKUnl7ObdC3j7+DfXuVGUua6fsMrX2fbZzn93veO9xsy0X576ucAaZZ36iQDH4iQLF4CcKFIOfKFAMfqJAMfiJAjWpVJ+IHATQj2LBtpyqrpqoTyFOmirWzEPPfnw3FyMNKN7Vh+0UlRgz3wBAPbepKft/k4J7/tisFvuuvsRIywFA+zx7/NOb7Fp3VyyrdW6/aLGdHxwanGG29Xbb//PxbvdybgCQHxl2bm9usI9v/4C7DwCkPfdZyrcUmTcdbPxvFU71lSPP/3VVPVaG2yGiBPFlP1GgJhv8CuAtEflQRDrKMSAiSsZkX/bfoKpHRKQNwNsi8ltVfXf8H0RPCnxiIDrPTOrMr6pHou89AF4GsNrxN+tUdVUpFwOJKDmxg19EmkSk5fTPAG4FsKNcAyOiyprMy/7ZAF6W4oy7GgD/rqpvTNTJtyRTecXcT5zJV57lnXwpR/WlFT3LWtUU7KKac2e7U0o33WD3ueYqz5Jcnv+tIVNvtl2+rNW5fWbbHLPPS784ara9s9Fezm3x/BazbVZLv3O76pDZ54Jp7jQlALTPskNmpif1Odpv39dj5tJyvrKfkxc7+FX1AICryjgWIkoQU31EgWLwEwWKwU8UKAY/UaAY/ESBSriA59Qm4p7hpr7F7sQuSumbDaieVN+lC+2ZdrfddoFz+zXX2YU4G2rsWWyjI/YsvGkt9mzA+kZ3+uroUXtW3Nv/1WW2/WqzXRwzW7BTbDf+2Wzn9vmzB80+telpZtv0mfZxPNo3Yrad3Gb3y2bdacBKJ8V55icKFIOfKFAMfqJAMfiJAsXgJwoUr/afRTxLg4lRh62x3n4OTXuO8NCAfbV/WrN9m7feal/tX3O7++r89Gn27eUH2sy2dI07ewAAuZx95X54yD1xpiD2pJnrfn+p2TaUtrMfN37NPYkIABavcB+rhhp7Ys/IkP1/zV9s32dXXmW37em0r933dfsmhlUOz/xEgWLwEwWKwU8UKAY/UaAY/ESBYvATBUo01lJYMXcmMdbCqgBfOs93OFqnu2vW3feXl5p96prtSTPr/2W/2XbJpc1m2/d/0GS2rbjcXc8uA7vO3YkuuxbfoYN2Xb3m+plmW2OTe/ztl1xs9jk1Zv/PvSfctfgAoHWaZwmt/OfO7WMj9v81nLUnY+mwfX/2dNn3y6M/+cRs+98tfc7tnoep93Gq6l0b7P/xzE8UKAY/UaAY/ESBYvATBYrBTxQoBj9RoCac1Sci6wF8E0CPqq6ItrUCeA7AIgAHAdytqr2l7bKkLERJPXxLf/n24nvG893mLbe4Z539dccSs0+mfp/Z1lJrz5jL29krzJljzx4bHXGPfzhn15A7dtKdagKAkyP2QGbPXWi2tc5yL8vVPGO62Wd6vZ2OXLzQTgMWxuz0W3Zsvnt71k71jYydMNv6j58y297f7E4rAsDBI3Z9Pxi1HNVcxqs8Sjnz/wzAmrO2PQhgo6ouBbAx+p2IppAJg19V3wVw9lPhHQA2RD9vAHBnmcdFRBUW9z3/bFXtAoDou10NgojOSxWv5CMiHQA6Kr0fIjo3cc/83SLSDgDRd3PxdFVdp6qrVHVVzH0RUQXEDf5XAayNfl4L4JXyDIeIklJKqu8ZADcDmCUinQAeBvBDAM+LyH0ADgH4dum7dKc1zj0BCIjYqRDxPK+JZ281afs229rct5nxZDlb6+wlqL51j52y+2yfPcaCsbwTAEDdBTzzhZzZpbbB3tesBe7lrgCgZaFdOHPuRUb6U+vMPuIp7plK2Q/VVMZeUmwU7hl6PccOm326O+3U4ZatJ822n79wwGw7fMyzbJtUJ9U3YfCr6r1G0zfKPBYiShA/4UcUKAY/UaAY/ESBYvATBYrBTxSo82atPl9lT6vgpvoKcRopxWKbZ906T0rs6Cn3zLjuk/boNW2PMVvTaLaNeWow1uftlFg67Z4p2NhoH49GO1OGoay9LuDwoJ0u6/nc3a+pwT3bDwAGB+wU28CQ+TkydB7uNtt2bXPP3tv06yNmn607j5ltnx+zZ+f1DdgzJ321a1XtdQgriWd+okAx+IkCxeAnChSDnyhQDH6iQDH4iQKVcKpPzLSdpD2z8FJGqi9lp6EamqaZbZka+98eG7FnbR3qcqdydh2wU3Y1Gbu4Z2/OTiv2nbTTP61z7WKWIu4imIPDA2af2lo7dTitwT5WfSc9x+rAFuf2sRHPDLxh+zEwMjpotg2O2ePIZd2zCD87ZBcm3bnP3pc/Je07l3rSeeaNVnZpS575iQLF4CcKFIOfKFAMfqJAMfiJApXw1X6FqvsK5rIVl5u9vvq1P3Buf/kVu25oc0OT2ZYdsydg5LL2VfHt2931+N5880Kzzw3X3mS2HT92yGyD2stC1dfb/1vayGT09tvLU6VHh8y2mjq75l59k2ccGXcmJpOyszDN9fPMNi3YGRWps8f/4ebjzu1PbvjY3pd41krznC41b0/G8tWUtK/q82o/EVUAg58oUAx+okAx+IkCxeAnChSDnyhQpSzXtR7ANwH0qOqKaNsjAP4KwOn80UOq+nppu3RPcKiptSfpzGxzp9IKWTtlNzBm14PLjtipocG+frPNKrV2cH+f2Wdk2E7/DJzyTUixbzOTmW62IeWu1Zcr2Meqps6zWFrGnkSUbrTHka533+boqD2O/oJdVy8j7tqEAHC025408/wvPnRu3/fpCbNPSj1hUbD3pZ7JOypxJvZUViln/p8BWOPY/iNVXRl9lRj4RHS+mDD4VfVdAPbTJBFNSZN5z3+/iGwTkfUiMqNsIyKiRMQN/icALAGwEkAXgMesPxSRDhHZJCKbYu6LiCogVvCrareq5rW42sBPAaz2/O06VV2lqqviDpKIyi9W8ItI+7hf7wKwozzDIaKklJLqewbAzQBmiUgngIcB3CwiK1FMUhwE8J1Sd5g28hp7du02+3T3utNeg8N2Dbz6OrsuXW1Lq9mWztvLWuWG3LO9eofsGXg9J+zZY83T6s22E55loUZzdtqotuB+Pm+bM9fs82mnPZPx0AF76apLl7vrBQJAW6uRmsvZM+aOH7eX/9q/Z5fZ9uKL7tmWALDxl+6lvHKe+zntCQtV+zHnrdPny+dZmdYKpwAnDH5Vvdex+ckKjIWIEsRP+BEFisFPFCgGP1GgGPxEgWLwEwUq4QKeQMHIX7TM9hTB/OM/cW7/aMtWs89Arz1jbvoMe4ZY/bA942/ASC3OvMxOX3163J1qAoAZF9qfe5ox9ytm2+iYPdWiMeuezVhXb8+a3H/APlZv//oTs23zbrvfVZdc4tw+LWMv13X8c/s4/s9/e9J5b9izAQfGjIe4sWwcAORhzzyMnX+r0sw9H575iQLF4CcKFIOfKFAMfqJAMfiJAsXgJwpUsqk+EWidO+U077LLzG5XrL7Ouf1on53y2tvfa7aNDtkpqhqxnw+nNblngk2fbs9uO3TYnun12lv2TOilC1eabXMbOs22Ky7LO7e3zbfTV9dea6dZ9x62Z0f+51v7zLaXn3rPuT2dtx9yOc8szcE+Ow2Yy9rpw5TxEC+oZz0++GbufXnwzE8UKAY/UaAY/ESBYvATBYrBTxSoxCf2SMF9xfzYgUNmnzeeetq5/eBeuz7esU779vJZz9Vca00uwFwKa+QT+2pz/xJ7SauPD9kZiS01dk3D+bPsK9XT117p3N7aZtcLzKfGzLa9vz1qtu36yM6a5EfdWQfk7IeceM5FNWahu4kq5xlZDvE9Bs7DWTgVwDM/UaAY/ESBYvATBYrBTxQoBj9RoBj8RIESnSCtISILADwFYA6KWZV1qvpjEWkF8ByARSgu2XW3qtq5q+JtaQrudFkh7lJHBl9qyJdS8nRDQd2Nvn3VwDOBpMUex1cW2HUGB4fs1NxXf2+Rc/u37nJvB4DOns/Mtn/+yR6zbf8ee0mxdL37fta8J2Xnyb6JJwWr3qWwYqTtpnimT9V4oJ6llDN/DsD3VXUZgOsBfFdElgN4EMBGVV0KYGP0OxFNERMGv6p2qerm6Od+ALsBzANwB4AN0Z9tAHBnpQZJROV3Tu/5RWQRgKsBvA9gtqp2AcUnCABt5R4cEVVOyR/vFZFmAC8CeEBVT4mn7vlZ/ToAdMQbHhFVSklnfhHJoBj4T6vqS9HmbhFpj9rbAfS4+qrqOlVdpar2ChVElLgJg1+Kp/gnAexW1cfHNb0KYG3081oAr5R/eERUKaWk+m4E8B6A7fjdBKqHUHzf/zyAiwAcAvBtVbWL6hVvS60Seb43ESmj1Td0b/onJiuBUpuuM/ukCvY7q+GUe2ktAGjIuFNlAJDP22mvC1rcNRKXXepJHQ7b9f06PTUIe472m23W/Vzw3Gnex2Lcu1PdAxEj5Vzcla+G3/mfByw11Tfhe35V/RXs2PzGuQyKiM4f/IQfUaAY/ESBYvATBYrBTxQoBj9RoCZM9ZV1ZyJqPt34hhFniKV9ALFMt+mbCuhJqKQ9MxnjHg81Cmd6NDXaxT2bGpvNtmPH7cxuwVcI1eK7z2I/TK2Unu+8F0aqj2d+okAx+IkCxeAnChSDnyhQDH6iQDH4iQKVfKqPqiKVsp/nUylPUc2CfZcVCjHSeVRxTPURkReDnyhQDH6iQDH4iQLF4CcKVMmlu2lq82V1crlyz6qiqYBnfqJAMfiJAsXgJwoUg58oUAx+okAx+IkCVcpafQtE5B0R2S0iO0Xke9H2R0TksIhsib5ur9QgReScv+hMqmp+FdN51hd9WZWyVl87gHZV3SwiLQA+BHAngLsBDKjqP5W8s5iz+uIEc5KzFYnOJ+Vcq68LQFf0c7+I7AYwb3LDI6JqO6f3/CKyCMDVKK7QCwD3i8g2EVkvIjPKPDYiqqCSg19EmgG8COABVT0F4AkASwCsRPGVwWNGvw4R2SQim8owXiIqk5Iq+YhIBsBrAN5U1ccd7YsAvKaqKya4Hb7nJ6qwslXykWLkPQlg9/jAjy4EnnYXgB3nOkgiqp5SrvbfCOA9ANsBnC7a9hCAe1F8ya8ADgL4TnRx0HdbZT0dx03p+f9lvmKgqa3UM/+ULuDJ4Cf6IhbwJCIvBj9RoBj8RIFi8BMFisFPFKgpUcAznU47t/uu9qvaS0n5rvb7lqci+jLhmZ8oUAx+okAx+IkCxeAnChSDnyhQDH6iQE2JVJ85+cgzfUE9E3RSnqe8VNpuFGOHubydVpxgFlEsLFVA5cAzP1GgGPxEgWLwEwWKwU8UKAY/UaAY/ESBmtKpPvGk89JpOw+Y8uX6vClCdz9fWjGXs9tiLynIVB+VAc/8RIFi8BMFisFPFCgGP1GgGPxEgZrwar+I1AN4F0Bd9PcvqOrDInIxgGcBtALYDOAvVHWskoM9W9wJLmnP5J1cLm+2ZbM553bfVfvaWnf9wYlks/Y4iMqhlDP/KIBbVPUqFNfmWyMi1wN4FMCPVHUpgF4A91VumERUbhMGvxYNRL9moi8FcAuAF6LtGwDcWZERElFFlPSeX0TSIrIFQA+AtwHsB9CnqqdfB3cCmFeZIRJRJZQU/KqaV9WVAOYDWA1gmevPXH1FpENENonIpvjDJKJyO6er/araB+CXAK4HcIGInL5gOB/AEaPPOlVdpaqrJjNQIiqvCYNfRC4UkQuinxsA/CGA3QDeAfCn0Z+tBfBKpQZJROUnZn28038gciWKF/TSKD5ZPK+q/yAii/G7VN9HAP5cVUcnuK1YyTlzIo7n5nxzd3ypPp+8UavPdwxranypPjtHODbmTisCgGclMiKoaklTxiYM/nJi8J+NwU/lV2rw8xN+RIFi8BMFisFPFCgGP1GgGPxEgUr6av9RAJ9Gv84CcCyxnds4jjNxHGeaauNYqKoXlnKDiQb/GTsW2XQ+fOqP4+A4Qh0HX/YTBYrBTxSoagb/uiruezyO40wcx5m+tOOo2nt+IqouvuwnClRVgl9E1ojIHhHZJyIPVmMM0TgOish2EdmSZLEREVkvIj0ismPctlYReVtE9kbfZ1RpHI+IyOHomGwRkdsTGMcCEXlHRHaLyE4R+V60PdFj4hlHosdEROpF5DcisjUax99H2y8Wkfej4/GciNROakeqmugXilOD9wNYDKAWwFYAy5MeRzSWgwBmVWG/NwG4BsCOcdv+EcCD0c8PAni0SuN4BMAPEj4e7QCuiX5uAfAxgOVJHxPPOBI9JihO92yOfs4AeB/FAjrPA7gn2v6vAP5mMvupxpl/NYB9qnpAi6W+nwVwRxXGUTWq+i6AE2dtvgPFuglAQgVRjXEkTlW7VHVz9HM/isVi5iHhY+IZR6K0qOJFc6sR/PMAfDbu92oW/1QAb4nIhyLSUaUxnDZbVbuA4oMQQFsVx3K/iGyL3hZU/O3HeCKyCMDVKJ7tqnZMzhoHkPAxSaJobjWC31VooFophxtU9RoAtwH4rojcVKVxnE+eALAExTUaugA8ltSORaQZwIsAHlDVU0ntt4RxJH5MdBJFc0tVjeDvBLBg3O9m8c9KU9Uj0fceAC+jeJCrpVtE2gEg+t5TjUGoanf0wCsA+CkSOiYikkEx4J5W1ZeizYkfE9c4qnVMon2fc9HcUlUj+D8AsDS6clkL4B4AryY9CBFpEpGW0z8DuBXADn+vinoVxUKoQBULop4OtshdSOCYiIgAeBLAblV9fFxTosfEGkfSxySxorlJXcE862rm7SheSd0P4G+rNIbFKGYatgLYmeQ4ADyD4svHLIqvhO4DMBPARgB7o++tVRrHzwFsB7ANxeBrT2AcN6L4EnYbgC3R1+1JHxPPOBI9JgCuRLEo7jYUn2j+btxj9jcA9gH4DwB1k9kPP+FHFCh+wo8oUAx+okAx+IkCxeAnChSDnyhQDH6iQDH4iQLF4CcK1P8BHYjtMsdMJyMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Firstly, we create the placeholder that will receive the test data\n",
    "x = tf.placeholder(tf.float32, [1, 32, 32, 3])\n",
    "\n",
    "\n",
    "# Since we used a dictionary instead of a list for storing the parameters, but the training routine returns\n",
    "# a list, we first convert manually the list into a dictionary\n",
    "update_params = {}\n",
    "list_of_keys = ['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4', 'W5','b5']\n",
    "update_params = {key : update_param[i] for i,key in enumerate(list_of_keys)}\n",
    "\n",
    "\n",
    "# We then, call the fordward function\n",
    "scores = five_layer_CNN(x=x, params=update_params, classes=10 ,training=False)\n",
    "\n",
    "idx = np.random.randint(10000)\n",
    "\n",
    "# Let's now create run the prediction graph\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 32, 32, 3)})\n",
    "    \n",
    "    print('The predicted class is: ', classes[np.argmax(scores2)])\n",
    "    print('The correct class is: ', classes[y_test[idx]])\n",
    "    \n",
    "    plt.imshow(x_test[idx].astype(np.int32))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_params = {}\n",
    "list_of_keys = ['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4', 'W5','b5']\n",
    "\n",
    "for i,key in enumerate(list_of_keys):\n",
    "    update_params[key] = update_param[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
