{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low level Tensorflow model for MNIST classification\n",
    "Ok, I understand that there are many really good Tensorflow tutorials out there. I have completed a couple myself. Particularly, I cannot recommend enought those from Stanford CS231n and Andrew Ng's Deep Learning Coursera. However, when I tried to create my first own project from scratch I found myself having to look for help in my previous sample homeworks from CS231n and Coursera. Thus, I decided that to get a real understanding of Tensorflow, I had to complete some projects from zero, without any sort of template.\n",
    "Thus, although this notebook is quite a selfish project to improve my proficiency, I reckon it may be of use to at least one individual out there in a similar situation, so feel free to use this code.\n",
    "For familiriaty, I will be using the ever useful MNIST database of handwritten characters.\n",
    "Please note that this Notebook assumes Deep Learning Understanding at a level of the courses mentioned above, as well as python, and is only a Tensorflow getting started tutorial from scratch.\n",
    "Also, please note that this notebook is the first of a 3-part series, the other two series will use CNNs and CIFAR10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing the data\n",
    "As I mentioned before I will be using MNIST for this notebook. \n",
    "We can use Tensorflow-Keras with the command 'tf.keras.datasets.mnist.load_data()' and download the database online, or we can download it from:\n",
    "https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "\n",
    "For more detailed information on the MNIST dataset, please refer to [LeCun et al., 1998a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998. Also please see http://yann.lecun.com/exdb/mnist/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datastrain, _ = tf.keras.datasets.mnist.load_data()\n",
    "#(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data('/tf/mydata/mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (60000, 28, 28)\n",
      "Train labels shape:  (60000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# It is a good idea to visualise the data we just loaded.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, as expected we have 60,000 training images and 10,0000 for testing. What is\n",
    "# each image shape? Let's find out\n",
    "print('Train image shape: ', x_train[1].shape)\n",
    "\n",
    "# Knowing it is a matrix shape, we can randomly show any of the numbers\n",
    "rnd_idx = np.random.randint(x_train.shape[0])\n",
    "plt.imshow(x_train[rnd_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and to use default TF settings, we cast the data to int32 and float32\n",
    "# Since, the dataset is relatively small, in most cases, this is not a problem\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types, not necessary, perhaps educational\n",
    "type(y_train)\n",
    "isinstance(y_train, np.ndarray) #validate if it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data are numpy arrays\n",
    "assert (isinstance(x_train, np.ndarray) and isinstance(y_train, np.ndarray)), 'No ndarray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a function to divide our data in N mini batches, this is an important step, to allow for mini batch \n",
    "# gradient descent\n",
    "\n",
    "def mini_batches(mini_batch_size, data_x, data_y = None):\n",
    "    # First we validate the data is of the expected type, and the number of labels meet the number of training samples\n",
    "    assert data_x.shape[0] == data_y.shape[0], 'X number of samples not equal to Y number of samples'\n",
    "    assert (isinstance(data_x, np.ndarray) and isinstance(data_y, np.ndarray)), 'Data not numpy array'\n",
    "    \n",
    "    N = data_x.shape[0] # Get the number of samples\n",
    "    idxs = np.arange(N) \n",
    "    # Shuffle data, this may not be so critical in this example, but it is important for most applications, to avoid\n",
    "    # strong correlations in mini batches\n",
    "    np.random.shuffle(idxs)\n",
    "    data_x = data_x[idxs] # Shuffle training samples\n",
    "    data_y = data_y[idxs] # Shuffle labels (don't forget)\n",
    "    \n",
    "    # Finally return the data in minibatches of the desired size\n",
    "    # List comprehension is so cool, technically this is returning a generator but the principle is the same\n",
    "    return ((data_x[i:i+mini_batch_size], data_y[i:i+mini_batch_size]) for i in range(0, N, mini_batch_size))\n",
    "\n",
    "type(mini_batches(64, x_test, y_test)) # Check type returned by function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:1].shape # In case we need to check the shape of one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if using list comprehension instead of generator\n",
    "# a = mini_batches(500, x_test, y_test)\n",
    "# print((a[5][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To iterate through the data, we use something like this\n",
    "for t,(x,y) in enumerate(mini_batches(64, x_train, y_train)):\n",
    "    print(t, x.shape, y.shape) # Print the shape for each minibatch\n",
    "    if t > 8:\n",
    "        break # Stop after the first 10 minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with tensorflow training\n",
    "\n",
    "In this notebook I show a very low level implementation, well not really I'm using Tensorflow, but still low level given TF standards. I reckon this is intuitive and shows what is going on, facilitating the transition to higher level TF and Keras in more complex projects.\n",
    "\n",
    "The approach followed in this notebook is to use functions to define the network architecture, and parameter initialization, as shown in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define the architecture of our FC networks.\n",
    "# I will start with a 4-Layer vanilla (tasty) FC NN with ReLu activations, weight decay regularization (L2),\n",
    "# using the number of hidden units given in, well, hidden_units\n",
    "\n",
    "\n",
    "def init_four_layer_FC(PIXELS, hidden_units):\n",
    "    '''\n",
    "    Initialise the parameters\n",
    "\n",
    "    Inputs:\n",
    "    - PIXELS: Scalar with total number of pixels in the image (for MNIST that would be 28x28 = 784)\n",
    "    - hidden_units: List with number of hidden neurons per layer\n",
    "\n",
    "    Outputs:\n",
    "    - Parameters: List with network parameters\n",
    "    '''\n",
    "    # Extract layer sizes\n",
    "    H1 = hidden_units[0]\n",
    "    H2 = hidden_units[1]\n",
    "    H3 = hidden_units[2]\n",
    "    classes = hidden_units[3]\n",
    "\n",
    "    #     Given that we only have 4 layers initializing with small random numbers should be fine, however we could\n",
    "    #     use a more roburs initialization like that in:   \n",
    "    #     He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,\n",
    "    #     ICCV 2015, https://arxiv.org/abs/1502.01852\n",
    "    #     For example:\n",
    "    '''\n",
    "    w1 = tf.Variable(tf.random_normal(\n",
    "                             (PIXELS, hidden_units1), dtype=tf.float32) * np.sqrt(2.0 / PIXELS), \n",
    "                             dtype=tf.float32)\n",
    "    '''\n",
    "    # First, let's define the weights, we use variables because they are maintained in the graph and can be \n",
    "    # mutated through iterations. If we manually declare the learnable weights, tf.Variables are usally the ones\n",
    "    # to use\n",
    "\n",
    "    # Define 1st layer parameters, note that tf.float32 is the default type, however I leave explicit in case\n",
    "    # we wanted to use different type of data for particular purposes\n",
    "\n",
    "    # Weights are left as normal random with std = 0.01 and mean = 0\n",
    "    # Biases are set to zero\n",
    "    # Since they are learnable parameters they are set to zero\n",
    "\n",
    "    w1 = tf.Variable(tf.random_normal((PIXELS, H1), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w1')\n",
    "    b1 = tf.Variable(tf.zeros(H1, dtype=tf.float32), dtype=tf.float32, name='b1')\n",
    "\n",
    "    w2 = tf.Variable(tf.random_normal((H1, H2), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w2')\n",
    "    b2 = tf.Variable(tf.zeros(H2, dtype=tf.float32), dtype=tf.float32, name='b2')\n",
    "\n",
    "    w3 = tf.Variable(tf.random_normal((H2, H3), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w3')\n",
    "    b3 = tf.Variable(tf.zeros(H3, dtype=tf.float32), dtype=tf.float32, name='b3')\n",
    "\n",
    "    w4 = tf.Variable(tf.random_normal((H3, classes), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w4')\n",
    "    b4 = tf.Variable(tf.zeros(classes, dtype=tf.float32), dtype=tf.float32, name='b4')\n",
    "\n",
    "    # Return parameters\n",
    "    parameters = [w1, b1, w2, b2, w3, b3, w4, b4]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def four_layer_FC(PIXELS,x, parameters):\n",
    "    '''\n",
    "    Create the inference graph, define the network architecture\n",
    "    \n",
    "    Inputs:\n",
    "    - PIXELS: Scalar with total number of pixels in the image (for MNIST that would be 28x28 = 784)\n",
    "    - x: Tensor with training or test images of shape (N, 28, 28) for MNIST\n",
    "    - parameters: Tuple with all the learnable weights and biases\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Verify that x Dimensions match \n",
    "    assert x.shape[1] * x.shape[2] == PIXELS, 'Image dimensions not as expected'\n",
    "    # Extract learning parameters\n",
    "    w1, b1, w2, b2, w3, b3, w4, b4 = parameters\n",
    "    # Implement architecture\n",
    "    x = tf.reshape(x,(-1, PIXELS))\n",
    "    \n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    a1 = tf.nn.relu(h1)\n",
    "    \n",
    "    h2 = tf.matmul(a1, w2) + b2\n",
    "    a2 = tf.nn.relu(h2)\n",
    "    \n",
    "    h3 = tf.matmul(a2, w3) + b3\n",
    "    a3 = tf.nn.relu(h3)\n",
    "    \n",
    "    scores = tf.matmul(a3, w4) + b4\n",
    "\n",
    "    return scores\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be sure that the model produces an expected output for a given input. Thus, the following step just veryfies that running data through produces a tensor of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function to test if the inference graph produces the expected output dimensions given an input\n",
    "# in this case the output should be of shape (N, 10)\n",
    "\n",
    "def test_FC(num_samples):\n",
    "    # Let us declare some useful constants\n",
    "    PIXELS = x_test.shape[1] * x_test.shape[2]\n",
    "    hidden_units = [100, 100, 100, 10]\n",
    "    \n",
    "    # Reset the default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "    \n",
    "    # Obtain parameters\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to the graph\n",
    "    scores = four_layer_FC(PIXELS, x, parameters)\n",
    "    \n",
    "    # Create session and run it\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        test = sess.run(scores, feed_dict={x:x_train[:num_samples]})\n",
    "        print(test.shape)\n",
    "\n",
    "# Test our model output size\n",
    "test_FC(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training model\n",
    "In this point we will start training our graph. For this we will use a Tensorflow session to run the inference graph and the training operations. This will look similiar to test_FC(), plus the required components to define a loss function and carry out learning step operations. For simplicity let us use vanilla Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines the complete training graph\n",
    "\n",
    "def train_FC_model(hidden_units, \n",
    "                   num_epochs=10,\n",
    "                   PIXELS=784,\n",
    "                   learning_rate=0.05,\n",
    "                   reg=0,\n",
    "                   print_every=100,\n",
    "                   minibatch_size = 64):\n",
    "    '''\n",
    "    Train Tensorflow model\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_units: List with number of hidden neurons per layer\n",
    "    - num_epochs: Integer with the number of epochs to run, an epoch is a complete pass in the whole training set\n",
    "    - PIXELS: Scalar with total number of pixels\n",
    "    - learning_rate: Float with the learning rate to use for updates, i.e. the step size towards the minimum\n",
    "    - reg: L2 regularization strength, default is set to 0 for no regularization\n",
    "    - print_every: This is a helping variable to stop during training and evaluate loss functions and accuracy\n",
    "    - minibatch_size: Integer with the number of elements in minibatch\n",
    "    \n",
    "    Outputs:\n",
    "    - updated_parameters: List with update parameters\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Reset default graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define training data placeholders using expected MNIST data dimensions\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28], name = 'x_train') # Training data\n",
    "    y = tf.placeholder(tf.int32,[None, ], name='y_train') # Labels\n",
    "    \n",
    "    # Add these placeholders to graph in saver, this will allow for easy model restore\n",
    "    tf.add_to_collection('images', x)\n",
    "    tf.add_to_collection('labels', y)\n",
    "\n",
    "    # Load parameters by running the previously defined function\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to graph using the function we coded, and the parameters\n",
    "    scores = four_layer_FC(PIXELS, x, parameters)\n",
    "    \n",
    "    # Save scores to model, this is key since it will allow using it for inference after restore\n",
    "    tf.add_to_collection('scores', scores)\n",
    "    \n",
    "    # Once the scores are computed, we need to obtain our Loss. Given the data, we will use Cross entropy \n",
    "    # Using Tensorflow implementation of Cross entropy\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=y) # Each sample loss\n",
    "    data_loss = tf.reduce_mean(losses, name='loss') # take minibatch average\n",
    "    \n",
    "    # Should we want to use l2 regularization, default is zero, i.e., no regularization\n",
    "    reg_loss = tf.reduce_mean(reg * (tf.nn.l2_loss(parameters[0])\n",
    "                                     + tf.nn.l2_loss(parameters[2])\n",
    "                                     + tf.nn.l2_loss(parameters[4])\n",
    "                                     + tf.nn.l2_loss(parameters[0])))\n",
    "    \n",
    "    loss = data_loss + reg_loss # Compute total loss\n",
    "    \n",
    "    # Up to this point we have computed the complete forward pass of the data. \n",
    "    # Since, this is 'low level' Tensorflow we need to compute the gradients for the loss \n",
    "    # w.r.t. all the learnable parameters. As I said, low level but still Tensorflow. So let's use tf.gradients!\n",
    "    grad_parameters = tf.gradients(loss, parameters) # Loss gradient w.r.t. parameters\n",
    "\n",
    "    # We need to update the weights manually, for this we can use tf.assign, or tf.assing_sub\n",
    "    # Using SGD, we need to update each parameter independently, so list comprehension works well\n",
    "    updated_parameters = [tf.assign_sub(w, learning_rate * grad)\n",
    "                          for w, grad in zip(parameters, grad_parameters)]\n",
    "    \n",
    "    tf.add_to_collection('weights', updated_parameters)\n",
    "    \n",
    "    accuracies = np.zeros(10) # Helping variable to store accuracies\n",
    "    losses = np.zeros(10) # Helping variable to store accuracies\n",
    "    \n",
    "    # Create saver to save the model\n",
    "    saver = tf.train.Saver()\n",
    "    # Now, recall we have only created the graph, in order to run it and train, we need a Session\n",
    "    with tf.Session() as sess:\n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model for num_epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            print('Epoch number: ', epoch) # Allow us to see what epoch we are running\n",
    "            \n",
    "            # Run the necessary iterations given the number of minibatches, this follows the function defined\n",
    "            # at the beginning of the notebook\n",
    "            for iteration,(x_mb, y_mb) in enumerate(mini_batches(minibatch_size, x_train, y_train)):\n",
    "                # The following line calculates the loss for the minibatch.\n",
    "                # Recall TF knows what is needed to run a function and will run accordinly, e.g. scores before the\n",
    "                # loss. Also note that we only need to feed the data needed into a placeholder. In this case, we \n",
    "                # feed the minibatch training samples and labels\n",
    "                loss_mb, update_param = sess.run([loss, updated_parameters], feed_dict={x:x_mb, y:y_mb})\n",
    "                \n",
    "                # The following condition allows a sanity check by printing accuracies and loss\n",
    "                if iteration % print_every == 0:\n",
    "                    # We define this function in the next cell\n",
    "                    accuracy = compute_accuracy(sess, minibatch_size, scores, x)\n",
    "                    accuracies[int(iteration/100)]=accuracy # save current accuracy\n",
    "                    losses[int(iteration/print_every)] = loss_mb\n",
    "                    print('Iteration: %d Loss: %f Accuracy: %f Learning rate: %f'\n",
    "                          %(iteration, loss_mb, accuracy, learning_rate))\n",
    "        \n",
    "        # We could use learning rate decay. There are several conditions we could test to define when to decay.\n",
    "        # E.g. we could use it when accuracy is over e.g. 90%, or the loss stays relatively flat\n",
    "            print('Last accs std: ', np.std(accuracies))\n",
    "            if np.std(accuracies) < 0.002:\n",
    "                learning_rate = 0.9 * learning_rate\n",
    "        # Save the \n",
    "        saver.save(sess, 'checkpoint_file')\n",
    "        \n",
    "    return update_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(sess, minibatch_size, scores, x):\n",
    "    '''\n",
    "    This function computes the accuracy of the current model\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: it needs a current tf.Session to run the scores\n",
    "    - minibatch_size: The size of the mini batch to run the scores\n",
    "    - scores: TF operation to run\n",
    "    - x: test data\n",
    "    \n",
    "    Outputs:\n",
    "    - acc: Accuracy\n",
    "    \n",
    "    '''\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    for it, (xtest_mb, ytest_mb) in enumerate(mini_batches(minibatch_size, x_test, y_test)):\n",
    "        scores_test = sess.run(scores, feed_dict={x:xtest_mb})\n",
    "        y_pred = np.argmax(scores_test, axis=1)\n",
    "\n",
    "#         In case we would like to compare some elements of the predicted and ground truth arrays\n",
    "#         if it % 200 == 0:\n",
    "#             print('y_pred: ', y_pred[:10])\n",
    "#             print('y_test: ', ytest_mb[:10])\n",
    "\n",
    "        num_samples += xtest_mb.shape[0]\n",
    "        num_correct += np.sum(np.equal(y_pred, ytest_mb))\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define number of neurons (units) in each hidden layer\n",
    "hidden_units = [100, 100, 100, 10]\n",
    "\n",
    "# Train the model and save weights\n",
    "updated_parameters =  train_FC_model(hidden_units=hidden_units, reg= 0.0001, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instruction is not needed for the tutorial but it is useful to know the variablse in the graph\n",
    "auxL = [op.name for op in tf.get_default_graph().get_operations() if op.op_def and op.op_def.name=='VariableV2']\n",
    "print(auxL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [100, 100, 100, 10]\n",
    "x = tf.placeholder(tf.float32, [1, 28, 28])\n",
    "#parameters = init_four_layer_FC(784, hidden_units)\n",
    "# Add scores\n",
    "scores = four_layer_FC(784, x, updated_parameters)    \n",
    "#eval_op = tf.nn.top_k(scores)\n",
    "\n",
    "idx = np.random.randint(10000)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    scores2 = sess.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    print('The predicted number is: ', np.argmax(scores2))\n",
    "    plt.imshow(x_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the previously trained model\n",
    "\n",
    "So far we have trained the complete model, computed its accuracy, and validated it with random samples. However, we did the whole process in one go. Since the dataset is relatively simple this is not an issue, nonetheless this would be prohibiting for more complex and larger datasets. Since we saved our trained model, we can just load it and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint_file\n",
      "The predited number is:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess2:\n",
    "    # Load saved Model\n",
    "    saver = tf.train.import_meta_graph('checkpoint_file.meta')\n",
    "    saver.restore(sess2, 'checkpoint_file')\n",
    "    \n",
    "    # Load saved points\n",
    "    # Scores includes the complete model, with final parameters\n",
    "    scores = tf.get_collection('scores')[0]\n",
    "    # Saved placeholders, since we only want to run the inference graph, we only load x\n",
    "    x = tf.get_collection('images')[0]\n",
    "    \n",
    "    # Choose radom point in test data\n",
    "    idx = np.random.randint(10000)\n",
    "    plt.imshow(x_test[idx])\n",
    "    \n",
    "    # Calculate the score\n",
    "    scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    print('The predited number is: ', np.argmax(scores2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
