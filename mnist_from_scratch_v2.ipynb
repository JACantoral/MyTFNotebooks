{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low level Tensorflow model for MNIST/CIFAR10 classification\n",
    "Ok, I understand that there are many really good Tensorflow tutorials out there. I have completed a couple myself. Particularly, I cannot recommend enought those from Stanford CS231n and Andrew Ng's Deep Learning Coursera. However, when I tried to create my first own project from scratch I found myself having to look for help in my previous sample homeworks from CS231n and Coursera. Thus, I decided that to get a real understanding of Tensorflow, I had to complete some projects from zero, without any sort of template.\n",
    "Although this notebook is quite a selfish project to improve my proficiency, I reckon it may be of use to at least one individual out there in a similar situation, so feel free to use this code.\n",
    "For familiriaty, I will be using the ever useful MNIST database of handwritten characters.\n",
    "Please note that this Notebook assumes Deep Learning Understanding at a level of the courses mentioned above, as well as python, and is only a Tensorflow getting started tutorial from scratch.\n",
    "Also, please note that this notebook is the first of a 3-part series, the other two series will use CNNs and CIFAR10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# If we are using tensorflow 2.0\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing the data\n",
    "As mentioned before, I will be using MNIST and CIFAR10 for this notebook. \n",
    "We can use Tensorflow-Keras with the command 'tf.keras.datasets.mnist.load_data()' and download the database online, or we can download it from:\n",
    "https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "For more detailed information on the MNIST dataset, please refer to [LeCun et al., 1998a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998. Also please see http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "In a second experiment, we will use the CIFAR10 dataset, which we will download from Alex Krizhevsky website at University of Toronto at https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "For more details of the CIFAR10, dataset please refer to https://www.cs.toronto.edu/~kriz/cifar.html or \n",
    "Krizhevsky, A., \"LearningMultipleLayersofFeaturesfromTinyImages\", 2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use MNIST\n",
    "# (x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test)= tf.keras.datasets.mnist.load_data('/tf/mydata/mnist.npz')\n",
    "\n",
    "# Uncomment to use CIFAR10\n",
    "# (x_train, y_train), (x_test, y_test)= tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Define dataset to use\n",
    "if x_test.shape[1] == 28:\n",
    "    dataset = 'MNIST'\n",
    "    PIXELS = 784\n",
    "else:\n",
    "    dataset = 'CIFAR10'\n",
    "    PIXELS = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (60000, 28, 28)\n",
      "Train labels shape:  (60000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# It is a good idea to visualise the data we just loaded.\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image shape:  (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f47368664a8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So, as expected we have 60,000 training images and 10,0000 for testing. What is\n",
    "# each image shape? Let's find out\n",
    "print('Train image shape: ', x_train[1].shape)\n",
    "\n",
    "# Knowing it is a matrix shape, we can randomly show any of the numbers\n",
    "rnd_idx = np.random.randint(x_train.shape[0])\n",
    "plt.imshow(x_train[rnd_idx].astype(np.int32)) #Using astype guarantess imshow work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity and to use default TF settings, we cast the data to int32 and float32\n",
    "# Since, the dataset is relatively small, in most cases, this is not a problem\n",
    "y_train = y_train.astype(np.int32).reshape(-1)\n",
    "y_test = y_test.astype(np.int32).reshape(-1)\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types, not necessary, perhaps educational\n",
    "type(y_train)\n",
    "isinstance(y_test, np.ndarray) #validate if it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data are numpy arrays\n",
    "assert (isinstance(x_train, np.ndarray) and isinstance(y_train, np.ndarray)), 'No ndarray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we define a function to divide our data in N mini batches, this is an important step, to allow for mini batch \n",
    "# gradient descent\n",
    "\n",
    "def mini_batches(mini_batch_size, data_x, data_y = None):\n",
    "    # First we validate the data is of the expected type, and the number of labels meet the number of training samples\n",
    "    assert data_x.shape[0] == data_y.shape[0], 'X number of samples not equal to Y number of samples'\n",
    "    assert (isinstance(data_x, np.ndarray) and isinstance(data_y, np.ndarray)), 'Data not numpy array'\n",
    "    \n",
    "    N = data_x.shape[0] # Get the number of samples\n",
    "    idxs = np.arange(N) \n",
    "    # Shuffle data, this may not be so critical in this example, but it is important for most applications, to avoid\n",
    "    # strong correlations in mini batches\n",
    "    np.random.shuffle(idxs)\n",
    "    data_x = data_x[idxs] # Shuffle training samples\n",
    "    data_y = data_y[idxs] # Shuffle labels (don't forget)\n",
    "    \n",
    "    # Finally return the data in minibatches of the desired size\n",
    "    # List comprehension is so cool, technically this is returning a generator but the principle is the same\n",
    "    return ((data_x[i:i+mini_batch_size], data_y[i:i+mini_batch_size]) for i in range(0, N, mini_batch_size))\n",
    "\n",
    "type(mini_batches(64, x_test, y_test)) # Check type returned by function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:1].shape # In case we need to check the shape of one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if using list comprehension instead of generator\n",
    "# a = mini_batches(500, x_test, y_test)\n",
    "# print((a[5][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 28, 28) (64,)\n",
      "1 (64, 28, 28) (64,)\n",
      "2 (64, 28, 28) (64,)\n",
      "3 (64, 28, 28) (64,)\n",
      "4 (64, 28, 28) (64,)\n",
      "5 (64, 28, 28) (64,)\n",
      "6 (64, 28, 28) (64,)\n",
      "7 (64, 28, 28) (64,)\n",
      "8 (64, 28, 28) (64,)\n",
      "9 (64, 28, 28) (64,)\n"
     ]
    }
   ],
   "source": [
    "# To iterate through the data, we use something like this\n",
    "for t,(x,y) in enumerate(mini_batches(64, x_train, y_train)):\n",
    "    print(t, x.shape, y.shape) # Print the shape for each minibatch\n",
    "    if t > 8:\n",
    "        break # Stop after the first 10 minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with tensorflow training\n",
    "\n",
    "In this notebook I show a very low level implementation, well not really I'm using Tensorflow, but still low level given TF standards. I reckon this is intuitive and shows what is going on, facilitating the transition to higher level TF and Keras in more complex projects.\n",
    "\n",
    "The approach followed in this notebook is to use functions to define the network architecture, and parameter initialization, as shown in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define the architecture of our FC networks.\n",
    "# I will start with a 4-Layer vanilla (tasty) FC NN with ReLu activations, weight decay regularization (L2),\n",
    "# using the number of hidden units given in, well, hidden_units\n",
    "\n",
    "\n",
    "def init_four_layer_FC(PIXELS, hidden_units):\n",
    "    '''\n",
    "    Initialise the parameters\n",
    "\n",
    "    Inputs:\n",
    "    - PIXELS: Scalar with total number of pixels in the image (for MNIST that would be 28x28 = 784)\n",
    "    - hidden_units: List with number of hidden neurons per layer\n",
    "\n",
    "    Outputs:\n",
    "    - Parameters: List with network parameters\n",
    "    '''\n",
    "    # Extract layer sizes\n",
    "    H1 = hidden_units[0]\n",
    "    H2 = hidden_units[1]\n",
    "    H3 = hidden_units[2]\n",
    "    classes = hidden_units[3]\n",
    "\n",
    "    #     Given that we only have 4 layers initializing with small random numbers should be fine, however we could\n",
    "    #     use a more roburs initialization like that in:   \n",
    "    #     He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,\n",
    "    #     ICCV 2015, https://arxiv.org/abs/1502.01852\n",
    "    #     For example:\n",
    "    '''\n",
    "    w1 = tf.Variable(tf.random_normal(\n",
    "                             (PIXELS, hidden_units1), dtype=tf.float32) * np.sqrt(2.0 / PIXELS), \n",
    "                             dtype=tf.float32)\n",
    "    '''\n",
    "    # First, let's define the weights, we use variables because they are maintained in the graph and can be \n",
    "    # mutated through iterations. If we manually declare the learnable weights, tf.Variables are usally the ones\n",
    "    # to use\n",
    "\n",
    "    # Define 1st layer parameters, note that tf.float32 is the default type, however I leave explicit in case\n",
    "    # we wanted to use different type of data for particular purposes\n",
    "\n",
    "    # Weights are left as normal random with std = 0.01 and mean = 0\n",
    "    # Biases are set to zero\n",
    "    # Since they are learnable parameters they are set to zero\n",
    "\n",
    "    w1 = tf.Variable(tf.random_normal((PIXELS, H1), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w1')\n",
    "    b1 = tf.Variable(tf.zeros(H1, dtype=tf.float32), dtype=tf.float32, name='b1')\n",
    "\n",
    "    w2 = tf.Variable(tf.random_normal((H1, H2), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w2')\n",
    "    b2 = tf.Variable(tf.zeros(H2, dtype=tf.float32), dtype=tf.float32, name='b2')\n",
    "\n",
    "    w3 = tf.Variable(tf.random_normal((H2, H3), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w3')\n",
    "    b3 = tf.Variable(tf.zeros(H3, dtype=tf.float32), dtype=tf.float32, name='b3')\n",
    "\n",
    "    w4 = tf.Variable(tf.random_normal((H3, classes), dtype=tf.float32) * 0.01, dtype=tf.float32, name='w4')\n",
    "    b4 = tf.Variable(tf.zeros(classes, dtype=tf.float32), dtype=tf.float32, name='b4')\n",
    "\n",
    "    # Return parameters\n",
    "    parameters = [w1, b1, w2, b2, w3, b3, w4, b4]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def four_layer_FC(PIXELS,x, parameters,dataset):\n",
    "    '''\n",
    "    Create the inference graph, define the network architecture\n",
    "    \n",
    "    Inputs:\n",
    "    - PIXELS: Scalar with total number of pixels in the image (for MNIST that would be 28x28 = 784)\n",
    "    - x: Tensor with training or test images of shape (N, 28, 28) for MNIST\n",
    "    - parameters: Tuple with all the learnable weights and biases\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Verify that x Dimensions match \n",
    "    if dataset == 'MNIST':\n",
    "        assert x.shape[1] * x.shape[2] == PIXELS, 'Image dimensions not as expected'\n",
    "    else:\n",
    "        assert x.shape[1] * x.shape[2] * x.shape[3] == PIXELS, 'Image dimensions not as expected'\n",
    "    # Extract learning parameters\n",
    "    w1, b1, w2, b2, w3, b3, w4, b4 = parameters\n",
    "    # Implement architecture\n",
    "    x = tf.reshape(x,(-1, PIXELS))\n",
    "    \n",
    "    h1 = tf.matmul(x, w1) + b1\n",
    "    a1 = tf.nn.relu(h1)\n",
    "    \n",
    "    h2 = tf.matmul(a1, w2) + b2\n",
    "    a2 = tf.nn.relu(h2)\n",
    "    \n",
    "    h3 = tf.matmul(a2, w3) + b3\n",
    "    a3 = tf.nn.relu(h3)\n",
    "    \n",
    "    scores = tf.matmul(a3, w4) + b4\n",
    "\n",
    "    return scores\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be sure that the model produces an expected output for a given input. Thus, the following step just veryfies that running data through produces a tensor of the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Help function to test if the inference graph produces the expected output dimensions given an input\n",
    "# in this case the output should be of shape (N, 10)\n",
    "\n",
    "def test_FC(num_samples, dataset):\n",
    "    # Let us declare some useful constants\n",
    "    if dataset == 'MNIST':\n",
    "        PIXELS = x_test.shape[1] * x_test.shape[2]\n",
    "    elif dataset == 'CIFAR10':\n",
    "        PIXELS = x_test.shape[1] * x_test.shape[2] * x_test.shape[3]\n",
    "    hidden_units = [100, 100, 100, 10]\n",
    "    \n",
    "    # Reset the default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define placeholder\n",
    "    if dataset == 'MNIST':\n",
    "        x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "    else:\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    \n",
    "    # Obtain parameters\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to the graph\n",
    "    scores = four_layer_FC(PIXELS, x, parameters, dataset)\n",
    "    \n",
    "    # Create session and run it\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        test = sess.run(scores, feed_dict={x:x_train[:num_samples]})\n",
    "        print(test.shape)\n",
    "\n",
    "# Test our model output size\n",
    "test_FC(10, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training model\n",
    "In this point we will start training our graph. For this we will use a Tensorflow session to run the inference graph and the training operations. This will look similiar to test_FC(), plus the required components to define a loss function and carry out learning step operations. For simplicity let us use vanilla Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines the complete training graph\n",
    "\n",
    "def train_FC_model(hidden_units, \n",
    "                   num_epochs=10,\n",
    "                   PIXELS=784,\n",
    "                   learning_rate=0.05,\n",
    "                   reg=0,\n",
    "                   print_every=100,\n",
    "                   minibatch_size = 64):\n",
    "    '''\n",
    "    Train Tensorflow model\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_units: List with number of hidden neurons per layer\n",
    "    - num_epochs: Integer with the number of epochs to run, an epoch is a complete pass in the whole training set\n",
    "    - PIXELS: Scalar with total number of pixels\n",
    "    - learning_rate: Float with the learning rate to use for updates, i.e. the step size towards the minimum\n",
    "    - reg: L2 regularization strength, default is set to 0 for no regularization\n",
    "    - print_every: This is a helping variable to stop during training and evaluate loss functions and accuracy\n",
    "    - minibatch_size: Integer with the number of elements in minibatch\n",
    "    \n",
    "    Outputs:\n",
    "    - updated_parameters: List with update parameters\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Reset default graph.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define training data placeholders using expected MNIST data dimensions\n",
    "    if dataset == 'MNIST':\n",
    "        x = tf.placeholder(tf.float32, [None, 28, 28], name = 'x_train') # Training data\n",
    "        y = tf.placeholder(tf.int32,[None, ], name='y_train') # Labels\n",
    "    elif dataset == 'CIFAR10':\n",
    "        x = tf.placeholder(tf.float32, [None, 32, 32, 3], name = 'x_train') # Training data\n",
    "        y = tf.placeholder(tf.int32, [None, ], name = 'y_train') # Training data\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add these placeholders to graph in saver, this will allow for easy model restore\n",
    "    tf.add_to_collection('images', x)\n",
    "    tf.add_to_collection('labels', y)\n",
    "\n",
    "    # Load parameters by running the previously defined function\n",
    "    parameters = init_four_layer_FC(PIXELS, hidden_units)\n",
    "    \n",
    "    # Add scores to graph using the function we coded, and the parameters\n",
    "    scores = four_layer_FC(PIXELS, x, parameters, dataset)\n",
    "    \n",
    "    # Save scores to model, this is key since it will allow using it for inference after restore\n",
    "    tf.add_to_collection('scores', scores)\n",
    "    \n",
    "    # Once the scores are computed, we need to obtain our Loss. Given the data, we will use Cross entropy \n",
    "    # Using Tensorflow implementation of Cross entropy\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=y) # Each sample loss\n",
    "    data_loss = tf.reduce_mean(losses, name='loss') # take minibatch average\n",
    "    \n",
    "    # Should we want to use l2 regularization, default is zero, i.e., no regularization\n",
    "    reg_loss = tf.reduce_mean(reg * (tf.nn.l2_loss(parameters[0])\n",
    "                                     + tf.nn.l2_loss(parameters[2])\n",
    "                                     + tf.nn.l2_loss(parameters[4])\n",
    "                                     + tf.nn.l2_loss(parameters[0])))\n",
    "    \n",
    "    loss = data_loss + reg_loss # Compute total loss\n",
    "    \n",
    "    # Up to this point we have computed the complete forward pass of the data. \n",
    "    # Since, this is 'low level' Tensorflow we need to compute the gradients for the loss \n",
    "    # w.r.t. all the learnable parameters. As I said, low level but still Tensorflow. So let's use tf.gradients!\n",
    "    grad_parameters = tf.gradients(loss, parameters) # Loss gradient w.r.t. parameters\n",
    "\n",
    "    # We need to update the weights manually, for this we can use tf.assign, or tf.assing_sub\n",
    "    # Using SGD, we need to update each parameter independently, so list comprehension works well\n",
    "    updated_parameters = [tf.assign_sub(w, learning_rate * grad)\n",
    "                          for w, grad in zip(parameters, grad_parameters)]\n",
    "    \n",
    "    tf.add_to_collection('weights', updated_parameters)\n",
    "    \n",
    "    accuracies = np.zeros(10) # Helping variable to store accuracies\n",
    "    losses = np.zeros(10) # Helping variable to store accuracies\n",
    "    \n",
    "    # Create saver to save the model\n",
    "    saver = tf.train.Saver()\n",
    "    # Now, recall we have only created the graph, in order to run it and train, we need a Session\n",
    "    with tf.Session() as sess:\n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model for num_epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            print('Epoch number: ', epoch) # Allow us to see what epoch we are running\n",
    "            \n",
    "            # Run the necessary iterations given the number of minibatches, this follows the function defined\n",
    "            # at the beginning of the notebook\n",
    "            for iteration,(x_mb, y_mb) in enumerate(mini_batches(minibatch_size, x_train, y_train)):\n",
    "                # The following line calculates the loss for the minibatch.\n",
    "                # Recall TF knows what is needed to run a function and will run accordinly, e.g. scores before the\n",
    "                # loss. Also note that we only need to feed the data needed into a placeholder. In this case, we \n",
    "                # feed the minibatch training samples and labels\n",
    "                loss_mb, update_param = sess.run([loss, updated_parameters], feed_dict={x:x_mb, y:y_mb})\n",
    "                \n",
    "                # The following condition allows a sanity check by printing accuracies and loss\n",
    "                if iteration % print_every == 0:\n",
    "                    # We define this function in the next cell\n",
    "                    accuracy = compute_accuracy(sess, minibatch_size, scores, x)\n",
    "                    accuracies[int(iteration/100)]=accuracy # save current accuracy\n",
    "                    losses[int(iteration/print_every)] = loss_mb\n",
    "                    print('Iteration: %d Loss: %f Accuracy: %f Learning rate: %f'\n",
    "                          %(iteration, loss_mb, accuracy, learning_rate))\n",
    "        \n",
    "        # We could use learning rate decay. There are several conditions we could test to define when to decay.\n",
    "        # E.g. we could use it when accuracy is over e.g. 90%, or the loss stays relatively flat\n",
    "            print('Last losses std: ', np.std(losses))\n",
    "            if np.std(losses) < 0.80:\n",
    "                learning_rate = 0.95 * learning_rate\n",
    "        # Save the \n",
    "        saver.save(sess, 'checkpoint_file')\n",
    "        \n",
    "    return update_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(sess, minibatch_size, scores, x):\n",
    "    '''\n",
    "    This function computes the accuracy of the current model\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: it needs a current tf.Session to run the scores\n",
    "    - minibatch_size: The size of the mini batch to run the scores\n",
    "    - scores: TF operation to run\n",
    "    - x: test data\n",
    "    \n",
    "    Outputs:\n",
    "    - acc: Accuracy\n",
    "    \n",
    "    '''\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "    for it, (xtest_mb, ytest_mb) in enumerate(mini_batches(minibatch_size, x_test, y_test)):\n",
    "        scores_test = sess.run(scores, feed_dict={x:xtest_mb})\n",
    "        y_pred = np.argmax(scores_test, axis=1)\n",
    "\n",
    "#         In case we would like to compare some elements of the predicted and ground truth arrays\n",
    "#         if it % 200 == 0:\n",
    "#             print('y_pred: ', y_pred[:10])\n",
    "#             print('y_test: ', ytest_mb[:10])\n",
    "\n",
    "        num_samples += xtest_mb.shape[0]\n",
    "        num_correct += np.sum(np.equal(y_pred, ytest_mb))\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number:  0\n",
      "Iteration: 0 Loss: 2.302840 Accuracy: 0.128500 Learning rate: 0.005000\n",
      "Iteration: 100 Loss: 2.287686 Accuracy: 0.267400 Learning rate: 0.005000\n",
      "Iteration: 200 Loss: 1.981783 Accuracy: 0.335500 Learning rate: 0.005000\n",
      "Iteration: 300 Loss: 0.923820 Accuracy: 0.734100 Learning rate: 0.005000\n",
      "Iteration: 400 Loss: 0.585860 Accuracy: 0.793700 Learning rate: 0.005000\n",
      "Last losses std:  0.9554509415252206\n",
      "Epoch number:  1\n",
      "Iteration: 0 Loss: 0.541987 Accuracy: 0.835000 Learning rate: 0.005000\n",
      "Iteration: 100 Loss: 0.435203 Accuracy: 0.865300 Learning rate: 0.005000\n",
      "Iteration: 200 Loss: 0.390653 Accuracy: 0.883000 Learning rate: 0.005000\n",
      "Iteration: 300 Loss: 0.439452 Accuracy: 0.890400 Learning rate: 0.005000\n",
      "Iteration: 400 Loss: 0.302634 Accuracy: 0.909300 Learning rate: 0.005000\n",
      "Last losses std:  0.2180107650741178\n",
      "Epoch number:  2\n",
      "Iteration: 0 Loss: 0.287759 Accuracy: 0.917600 Learning rate: 0.004750\n",
      "Iteration: 100 Loss: 0.324634 Accuracy: 0.916500 Learning rate: 0.004750\n",
      "Iteration: 200 Loss: 0.207634 Accuracy: 0.927400 Learning rate: 0.004750\n",
      "Iteration: 300 Loss: 0.192326 Accuracy: 0.935900 Learning rate: 0.004750\n",
      "Iteration: 400 Loss: 0.169175 Accuracy: 0.940300 Learning rate: 0.004750\n",
      "Last losses std:  0.12542444766562116\n",
      "Epoch number:  3\n",
      "Iteration: 0 Loss: 0.174644 Accuracy: 0.940700 Learning rate: 0.004513\n",
      "Iteration: 100 Loss: 0.242494 Accuracy: 0.944100 Learning rate: 0.004513\n",
      "Iteration: 200 Loss: 0.128004 Accuracy: 0.947300 Learning rate: 0.004513\n",
      "Iteration: 300 Loss: 0.144923 Accuracy: 0.952300 Learning rate: 0.004513\n",
      "Iteration: 400 Loss: 0.137777 Accuracy: 0.953200 Learning rate: 0.004513\n",
      "Last losses std:  0.08783046143543109\n",
      "Epoch number:  4\n",
      "Iteration: 0 Loss: 0.135365 Accuracy: 0.952400 Learning rate: 0.004287\n",
      "Iteration: 100 Loss: 0.088452 Accuracy: 0.952800 Learning rate: 0.004287\n",
      "Iteration: 200 Loss: 0.060971 Accuracy: 0.957800 Learning rate: 0.004287\n",
      "Iteration: 300 Loss: 0.126120 Accuracy: 0.959300 Learning rate: 0.004287\n",
      "Iteration: 400 Loss: 0.111385 Accuracy: 0.959400 Learning rate: 0.004287\n",
      "Last losses std:  0.055586754176613254\n",
      "Epoch number:  5\n",
      "Iteration: 0 Loss: 0.125031 Accuracy: 0.958500 Learning rate: 0.004073\n",
      "Iteration: 100 Loss: 0.073558 Accuracy: 0.962300 Learning rate: 0.004073\n",
      "Iteration: 200 Loss: 0.081843 Accuracy: 0.964200 Learning rate: 0.004073\n",
      "Iteration: 300 Loss: 0.070719 Accuracy: 0.961800 Learning rate: 0.004073\n",
      "Iteration: 400 Loss: 0.100885 Accuracy: 0.963700 Learning rate: 0.004073\n",
      "Last losses std:  0.04742051542506649\n",
      "Epoch number:  6\n",
      "Iteration: 0 Loss: 0.124324 Accuracy: 0.960700 Learning rate: 0.003869\n",
      "Iteration: 100 Loss: 0.082989 Accuracy: 0.962700 Learning rate: 0.003869\n",
      "Iteration: 200 Loss: 0.084887 Accuracy: 0.966000 Learning rate: 0.003869\n",
      "Iteration: 300 Loss: 0.052460 Accuracy: 0.960300 Learning rate: 0.003869\n",
      "Iteration: 400 Loss: 0.176826 Accuracy: 0.965400 Learning rate: 0.003869\n",
      "Last losses std:  0.060310945237909044\n",
      "Epoch number:  7\n",
      "Iteration: 0 Loss: 0.094902 Accuracy: 0.964300 Learning rate: 0.003675\n",
      "Iteration: 100 Loss: 0.089577 Accuracy: 0.966400 Learning rate: 0.003675\n",
      "Iteration: 200 Loss: 0.046805 Accuracy: 0.968800 Learning rate: 0.003675\n",
      "Iteration: 300 Loss: 0.127135 Accuracy: 0.965100 Learning rate: 0.003675\n",
      "Iteration: 400 Loss: 0.159256 Accuracy: 0.970600 Learning rate: 0.003675\n",
      "Last losses std:  0.05826511424061984\n",
      "Epoch number:  8\n",
      "Iteration: 0 Loss: 0.087764 Accuracy: 0.968200 Learning rate: 0.003492\n",
      "Iteration: 100 Loss: 0.061216 Accuracy: 0.970100 Learning rate: 0.003492\n",
      "Iteration: 200 Loss: 0.098959 Accuracy: 0.969000 Learning rate: 0.003492\n",
      "Iteration: 300 Loss: 0.132654 Accuracy: 0.969400 Learning rate: 0.003492\n",
      "Iteration: 400 Loss: 0.175747 Accuracy: 0.964100 Learning rate: 0.003492\n",
      "Last losses std:  0.06226993896547086\n",
      "Epoch number:  9\n",
      "Iteration: 0 Loss: 0.082599 Accuracy: 0.970300 Learning rate: 0.003317\n",
      "Iteration: 100 Loss: 0.080708 Accuracy: 0.970800 Learning rate: 0.003317\n",
      "Iteration: 200 Loss: 0.104746 Accuracy: 0.968200 Learning rate: 0.003317\n",
      "Iteration: 300 Loss: 0.128467 Accuracy: 0.960100 Learning rate: 0.003317\n",
      "Iteration: 400 Loss: 0.040407 Accuracy: 0.970100 Learning rate: 0.003317\n",
      "Last losses std:  0.048325981573901125\n",
      "Epoch number:  10\n",
      "Iteration: 0 Loss: 0.057613 Accuracy: 0.971900 Learning rate: 0.003151\n",
      "Iteration: 100 Loss: 0.023773 Accuracy: 0.973400 Learning rate: 0.003151\n",
      "Iteration: 200 Loss: 0.083004 Accuracy: 0.970900 Learning rate: 0.003151\n",
      "Iteration: 300 Loss: 0.042533 Accuracy: 0.973000 Learning rate: 0.003151\n",
      "Iteration: 400 Loss: 0.040012 Accuracy: 0.973600 Learning rate: 0.003151\n",
      "Last losses std:  0.028436618486010937\n",
      "Epoch number:  11\n",
      "Iteration: 0 Loss: 0.056694 Accuracy: 0.972700 Learning rate: 0.002994\n",
      "Iteration: 100 Loss: 0.045503 Accuracy: 0.972200 Learning rate: 0.002994\n",
      "Iteration: 200 Loss: 0.041330 Accuracy: 0.971000 Learning rate: 0.002994\n",
      "Iteration: 300 Loss: 0.027482 Accuracy: 0.973100 Learning rate: 0.002994\n",
      "Iteration: 400 Loss: 0.081080 Accuracy: 0.970200 Learning rate: 0.002994\n",
      "Last losses std:  0.028226244842655988\n",
      "Epoch number:  12\n",
      "Iteration: 0 Loss: 0.078942 Accuracy: 0.973300 Learning rate: 0.002844\n",
      "Iteration: 100 Loss: 0.123773 Accuracy: 0.969400 Learning rate: 0.002844\n",
      "Iteration: 200 Loss: 0.072067 Accuracy: 0.972600 Learning rate: 0.002844\n",
      "Iteration: 300 Loss: 0.091643 Accuracy: 0.971900 Learning rate: 0.002844\n",
      "Iteration: 400 Loss: 0.069856 Accuracy: 0.975000 Learning rate: 0.002844\n",
      "Last losses std:  0.04581427317896629\n",
      "Epoch number:  13\n",
      "Iteration: 0 Loss: 0.038611 Accuracy: 0.973800 Learning rate: 0.002702\n",
      "Iteration: 100 Loss: 0.064216 Accuracy: 0.974000 Learning rate: 0.002702\n",
      "Iteration: 200 Loss: 0.033429 Accuracy: 0.973700 Learning rate: 0.002702\n",
      "Iteration: 300 Loss: 0.016735 Accuracy: 0.973100 Learning rate: 0.002702\n",
      "Iteration: 400 Loss: 0.026993 Accuracy: 0.973600 Learning rate: 0.002702\n",
      "Last losses std:  0.02121616000429482\n",
      "Epoch number:  14\n",
      "Iteration: 0 Loss: 0.033568 Accuracy: 0.972400 Learning rate: 0.002567\n",
      "Iteration: 100 Loss: 0.025404 Accuracy: 0.973600 Learning rate: 0.002567\n",
      "Iteration: 200 Loss: 0.014616 Accuracy: 0.971700 Learning rate: 0.002567\n",
      "Iteration: 300 Loss: 0.049356 Accuracy: 0.975000 Learning rate: 0.002567\n",
      "Iteration: 400 Loss: 0.007234 Accuracy: 0.973700 Learning rate: 0.002567\n",
      "Last losses std:  0.016671951138365875\n",
      "Epoch number:  15\n",
      "Iteration: 0 Loss: 0.008807 Accuracy: 0.974200 Learning rate: 0.002438\n",
      "Iteration: 100 Loss: 0.015880 Accuracy: 0.973400 Learning rate: 0.002438\n",
      "Iteration: 200 Loss: 0.010541 Accuracy: 0.974300 Learning rate: 0.002438\n",
      "Iteration: 300 Loss: 0.033811 Accuracy: 0.974500 Learning rate: 0.002438\n",
      "Iteration: 400 Loss: 0.018719 Accuracy: 0.974600 Learning rate: 0.002438\n",
      "Last losses std:  0.010790184118354421\n",
      "Epoch number:  16\n",
      "Iteration: 0 Loss: 0.017671 Accuracy: 0.975400 Learning rate: 0.002316\n",
      "Iteration: 100 Loss: 0.049662 Accuracy: 0.974600 Learning rate: 0.002316\n",
      "Iteration: 200 Loss: 0.022118 Accuracy: 0.974300 Learning rate: 0.002316\n",
      "Iteration: 300 Loss: 0.081878 Accuracy: 0.975300 Learning rate: 0.002316\n",
      "Iteration: 400 Loss: 0.020657 Accuracy: 0.974600 Learning rate: 0.002316\n",
      "Last losses std:  0.025908903157055817\n",
      "Epoch number:  17\n",
      "Iteration: 0 Loss: 0.046282 Accuracy: 0.975200 Learning rate: 0.002201\n",
      "Iteration: 100 Loss: 0.015633 Accuracy: 0.975900 Learning rate: 0.002201\n",
      "Iteration: 200 Loss: 0.014285 Accuracy: 0.975800 Learning rate: 0.002201\n",
      "Iteration: 300 Loss: 0.032034 Accuracy: 0.974900 Learning rate: 0.002201\n",
      "Iteration: 400 Loss: 0.040282 Accuracy: 0.974900 Learning rate: 0.002201\n",
      "Last losses std:  0.017417339281885474\n",
      "Epoch number:  18\n",
      "Iteration: 0 Loss: 0.021042 Accuracy: 0.976000 Learning rate: 0.002091\n",
      "Iteration: 100 Loss: 0.008034 Accuracy: 0.974600 Learning rate: 0.002091\n",
      "Iteration: 200 Loss: 0.023564 Accuracy: 0.974100 Learning rate: 0.002091\n",
      "Iteration: 300 Loss: 0.007421 Accuracy: 0.974700 Learning rate: 0.002091\n",
      "Iteration: 400 Loss: 0.032420 Accuracy: 0.974000 Learning rate: 0.002091\n",
      "Last losses std:  0.011460527370606622\n",
      "Epoch number:  19\n",
      "Iteration: 0 Loss: 0.026841 Accuracy: 0.973700 Learning rate: 0.001986\n",
      "Iteration: 100 Loss: 0.015649 Accuracy: 0.975800 Learning rate: 0.001986\n",
      "Iteration: 200 Loss: 0.044049 Accuracy: 0.974300 Learning rate: 0.001986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300 Loss: 0.022547 Accuracy: 0.975900 Learning rate: 0.001986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1105 23:44:01.037076 139945902630720 meta_graph.py:449] Issue encountered when serializing weights.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 400 Loss: 0.010407 Accuracy: 0.976000 Learning rate: 0.001986\n",
      "Last losses std:  0.014472220690213014\n"
     ]
    }
   ],
   "source": [
    "# Define number of neurons (units) in each hidden layer\n",
    "hidden_units = [100, 100, 100, 10]\n",
    "\n",
    "# Train the model and save weights\n",
    "updated_parameters =  train_FC_model(hidden_units=hidden_units, \n",
    "                                     num_epochs=20,\n",
    "                                     PIXELS=PIXELS, \n",
    "                                     learning_rate=0.005,\n",
    "                                     minibatch_size=128,\n",
    "                                     reg= 0.0, \n",
    "                                     print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4']\n"
     ]
    }
   ],
   "source": [
    "# This instruction is not needed for the tutorial but it is useful to know the variablse in the graph\n",
    "auxL = [op.name for op in tf.get_default_graph().get_operations() if op.op_def and op.op_def.name=='VariableV2']\n",
    "print(auxL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is:  Eight\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_units = [100, 100, 100, 10]\n",
    "if dataset == 'MNIST':\n",
    "    x = tf.placeholder(tf.float32, [1, 28, 28])\n",
    "    classes = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five',\n",
    "              'Six', 'Seven', 'Eight', 'Nine']\n",
    "elif dataset == 'CIFAR10':\n",
    "    x = tf.placeholder(tf.float32, [1, 32, 32, 3])\n",
    "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "              'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "#parameters = init_four_layer_FC(784, hidden_units)\n",
    "# Add scores\n",
    "scores = four_layer_FC(PIXELS, x, updated_parameters,dataset)    \n",
    "#eval_op = tf.nn.top_k(scores)\n",
    "\n",
    "idx = np.random.randint(10000)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if dataset=='MNIST':\n",
    "        scores2 = sess.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    else:\n",
    "        scores2 = sess.run(scores, feed_dict={x:x_test[idx].reshape(1, 32, 32, 3)})\n",
    "        \n",
    "    print('The predicted class is: ', classes[np.argmax(scores2)])\n",
    "    plt.imshow(x_test[idx].astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the previously trained model\n",
    "\n",
    "So far we have trained the complete model, computed its accuracy, and validated it with random samples. However, we did the whole process in one go. Since the dataset is relatively simple this is not an issue, nonetheless this would be prohibiting for more complex and larger datasets. Since we saved our trained model, we can just load it and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    x = tf.placeholder(tf.float32, [1, 28, 28])\n",
    "    classes = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five',\n",
    "              'Six', 'Seven', 'Eight', 'Nine']\n",
    "elif dataset == 'CIFAR10':\n",
    "    x = tf.placeholder(tf.float32, [1, 32, 32, 3])\n",
    "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "              'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    # Load saved Model\n",
    "    saver = tf.train.import_meta_graph('checkpoint_file.meta')\n",
    "    saver.restore(sess2, 'checkpoint_file')\n",
    "    \n",
    "    # Load saved points\n",
    "    # Scores includes the complete model, with final parameters\n",
    "    scores = tf.get_collection('scores')[0]\n",
    "    # Saved placeholders, since we only want to run the inference graph, we only load x\n",
    "    x = tf.get_collection('images')[0]\n",
    "    \n",
    "    # Choose radom point in test data\n",
    "    idx = np.random.randint(10000)\n",
    "    plt.imshow(x_test[idx].astype(np.int32))\n",
    "    \n",
    "    # Calculate the score\n",
    "    if dataset=='MNIST':\n",
    "        scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 28, 28)})\n",
    "    else:\n",
    "        scores2 = sess2.run(scores, feed_dict={x:x_test[idx].reshape(1, 32, 32, 3)})\n",
    "        \n",
    "    print('The predited number is: ', classes[np.argmax(scores2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
